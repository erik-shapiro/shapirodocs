{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Shapiro MIS Documentation Ocean Insights Shapiro Subscription Program: imp/callOceanInsightsTracking.p Check OI data OI API Data Carrier List Ports America Shapiro Subscription URL Shapiro Response URL Document Indexing Hub 360 Shapiro Response URL Regen The WSDL How to regen the WSDL Database Changes Crontab Changes Change the crontab by typing: crontab -e -u demon1 Reset The Demons cd to /usr/bin run stopdemons cd to /data3/demon1 remove STOPDEMON1 run new.start.demon as su Local Documentation Setup Download Git . Install Python (be sure to \"Add Python 3.7 to PATH\"). It should include pip by default. Go into your favorite directory. Right-click in whitespace and click \"Git Bash Here\". A command prompt will come up. Run the following: pip install mkdocs (this will install mkdocs, which you need to compile the documentation into HTML pages) git clone //shap-exp-360pro/c$/inetpub/wwwroot/shapiro-cms/templates/rt_metamorph/shapirodocumentaion cd shapirodocumentaion git checkout master Now make your changes. Once you are done, commit your raw changes to the master branch: git pull (make sure you have the latest version of the repository) git status (make sure your files show up as edited) git add {filename} (stage your files for a commit) git commit (be sure to add a commit message on the next screen, or else your commit will not take) git push (this pushes your raw changes to the master branch) mkdocs gh-deploy (this compiles the documentation site and syncs it to 360) Please remember to do both: push to the master branch and deploy the new site. These are two distinct branches, and updating one will not update the other. Miscellaneous Tickets Reports M+R ASN New Shapiro 360 Login Database Class Lost ISF If someone transmits an ISF, and the file gets nuked, find the ABI response using abitoday and the BL number. A1303655ROBERT113018 SN vvvvvvvvv B 1601655SN vvvvvvvvv SF10101ACTEI wwwwwwwwwwww 10xxxxxxxxxxxxxxx zzzzzzzzzzzz 018 SF15BMyyyyyyyyyyyyyyy SF9002 ISF ACCEPTED Y 1601655SN00004 Z1303655 113018 xxxxxxxxxxxxxxx - transaction number yyyyyyyyyyyyyyy - BL number vvvvvvvvv - batch number The user will need to create a new file and fill out the ISF module. Don't retransmit the ISF, though. Put the transaction number in a note on the new file like this: Note type - \"A\" Note login - \"ABI\" Note text - \"ISF ACCEPTED - \" + {transaction number} And put it in isf-header.isf-trans-no. test","title":"Shapiro MIS Documentation"},{"location":"#shapiro-mis-documentation","text":"","title":"Shapiro MIS Documentation"},{"location":"#ocean-insights","text":"Shapiro Subscription Program: imp/callOceanInsightsTracking.p Check OI data OI API Data Carrier List","title":"Ocean Insights"},{"location":"#ports-america","text":"Shapiro Subscription URL Shapiro Response URL","title":"Ports America"},{"location":"#document-indexing","text":"","title":"Document Indexing"},{"location":"#hub-360","text":"Shapiro Response URL","title":"Hub 360"},{"location":"#regen-the-wsdl","text":"How to regen the WSDL","title":"Regen The WSDL"},{"location":"#database-changes","text":"","title":"Database Changes"},{"location":"#crontab-changes","text":"Change the crontab by typing: crontab -e -u demon1","title":"Crontab Changes"},{"location":"#reset-the-demons","text":"cd to /usr/bin run stopdemons cd to /data3/demon1 remove STOPDEMON1 run new.start.demon as su","title":"Reset The Demons"},{"location":"#local-documentation-setup","text":"Download Git . Install Python (be sure to \"Add Python 3.7 to PATH\"). It should include pip by default. Go into your favorite directory. Right-click in whitespace and click \"Git Bash Here\". A command prompt will come up. Run the following: pip install mkdocs (this will install mkdocs, which you need to compile the documentation into HTML pages) git clone //shap-exp-360pro/c$/inetpub/wwwroot/shapiro-cms/templates/rt_metamorph/shapirodocumentaion cd shapirodocumentaion git checkout master Now make your changes. Once you are done, commit your raw changes to the master branch: git pull (make sure you have the latest version of the repository) git status (make sure your files show up as edited) git add {filename} (stage your files for a commit) git commit (be sure to add a commit message on the next screen, or else your commit will not take) git push (this pushes your raw changes to the master branch) mkdocs gh-deploy (this compiles the documentation site and syncs it to 360) Please remember to do both: push to the master branch and deploy the new site. These are two distinct branches, and updating one will not update the other.","title":"Local Documentation Setup"},{"location":"#miscellaneous","text":"Tickets Reports M+R ASN New Shapiro 360 Login Database Class","title":"Miscellaneous"},{"location":"#lost-isf","text":"If someone transmits an ISF, and the file gets nuked, find the ABI response using abitoday and the BL number. A1303655ROBERT113018 SN vvvvvvvvv B 1601655SN vvvvvvvvv SF10101ACTEI wwwwwwwwwwww 10xxxxxxxxxxxxxxx zzzzzzzzzzzz 018 SF15BMyyyyyyyyyyyyyyy SF9002 ISF ACCEPTED Y 1601655SN00004 Z1303655 113018 xxxxxxxxxxxxxxx - transaction number yyyyyyyyyyyyyyy - BL number vvvvvvvvv - batch number The user will need to create a new file and fill out the ISF module. Don't retransmit the ISF, though. Put the transaction number in a note on the new file like this: Note type - \"A\" Note login - \"ABI\" Note text - \"ISF ACCEPTED - \" + {transaction number} And put it in isf-header.isf-trans-no. test","title":"Lost ISF"},{"location":"360-login/","text":"New Shapiro 360 Login These are instructions on how to set up a 360 user. To query this information, you will need to query the MySQL database that Joomla saves into. (MySQL Workbench is nice.) Log into the Shapiro 360 admin: https://shapiro360.shapiro.com/administrator At the top, select Users -> Manage -> Add New User. General tab: Cust No \u2013 A comma-delimited list of customer codes. Name \u2013 The customer code, a dash, and their name. For example: \u201cJOIEJEAN - John Doe\u201d. Login Name \u2013 Typically, the user\u2019s email address. If you are setting up a test account for internal use, this will be the customer code in lowercase. Password \u2013 Make something up. This thing is nice: https://www.warpconduit.net/password-generator/ Confirm Password \u2013 Use the same password. Email \u2013 The user\u2019s email address. If you are setting up a test account for internal use, this will be the customer code @shapiro.com. For example: \u201cjoiejean@shapiro.com\u201d. Send Email \u2013 You can check this if you like. It will send an email to the user containing their credentials. Ecomm wants you not to do this. Block this User \u2013 Flip to \u201cyes\u201d to ban a user. Copy to Clipboard \u2013 This copies the user credential form into your clipboard (if you are using Firefox or Chrome). You can use this to paste (Ctrl +V) it into an email to Ecomm. Web Login Fields tab: Web Login Fields Charge Cust \u2013 If the user has multiple customers, and they need Online Payment, you will have to set this to one customer. Purchase Order Management Cust No \u2013 The customer code for the POs this user should see. Typically the same as the \u201cCust No\u201d on the first tab. Roles \u2013 \u201cCUSTOMER\u201d, \u201cSUPPLIER\u201d, \u201cAGENT\u201d, \u201cSHAPIRO\u201d Vendors \u2013 This should match the vendor code in a cust-po record (cust-po.vendor). If it\u2019s blank, then the user will see all POs for the customer. Agent Cust No \u2013 The agent\u2019s code in our system. This will go in shiphead.foreign-agent. Email Booking Details to Address \u2013 If this user makes a change to a booking in POM Booking, then the update will be emailed to the email address specified here. Booking Approval Fields Booking Approval Cust-No \u2013 The customer code for the bookings this user should see. Typically the same as the \u201cCust No\u201d on the first tab. Allow Shipment Approval \u2013 Yes if the user should be able to move bookings from pending to approved. Allow Shipment Notes \u2013 Yes if the user should be able to add notes to a booking. Allow Delivery Location Change \u2013 Yes if the user should be able to change the delivery location on a booking. General Access tab: App Code Access If it\u2019s an ecomm user, click \u201cEcomm User\u201d. This will set everything you need. Otherwise, see if a user already exists for the customer. If so, copy permissions from them. If you can\u2019t do either of the above, then go back to the person who requested this user and ask what access they should have. Reports Determine which reports the user should be able to see on the Reports page. POM Access tab: These are PO-related pages.","title":"New Shapiro 360 Login"},{"location":"360-login/#new-shapiro-360-login","text":"These are instructions on how to set up a 360 user. To query this information, you will need to query the MySQL database that Joomla saves into. (MySQL Workbench is nice.) Log into the Shapiro 360 admin: https://shapiro360.shapiro.com/administrator At the top, select Users -> Manage -> Add New User. General tab: Cust No \u2013 A comma-delimited list of customer codes. Name \u2013 The customer code, a dash, and their name. For example: \u201cJOIEJEAN - John Doe\u201d. Login Name \u2013 Typically, the user\u2019s email address. If you are setting up a test account for internal use, this will be the customer code in lowercase. Password \u2013 Make something up. This thing is nice: https://www.warpconduit.net/password-generator/ Confirm Password \u2013 Use the same password. Email \u2013 The user\u2019s email address. If you are setting up a test account for internal use, this will be the customer code @shapiro.com. For example: \u201cjoiejean@shapiro.com\u201d. Send Email \u2013 You can check this if you like. It will send an email to the user containing their credentials. Ecomm wants you not to do this. Block this User \u2013 Flip to \u201cyes\u201d to ban a user. Copy to Clipboard \u2013 This copies the user credential form into your clipboard (if you are using Firefox or Chrome). You can use this to paste (Ctrl +V) it into an email to Ecomm. Web Login Fields tab: Web Login Fields Charge Cust \u2013 If the user has multiple customers, and they need Online Payment, you will have to set this to one customer. Purchase Order Management Cust No \u2013 The customer code for the POs this user should see. Typically the same as the \u201cCust No\u201d on the first tab. Roles \u2013 \u201cCUSTOMER\u201d, \u201cSUPPLIER\u201d, \u201cAGENT\u201d, \u201cSHAPIRO\u201d Vendors \u2013 This should match the vendor code in a cust-po record (cust-po.vendor). If it\u2019s blank, then the user will see all POs for the customer. Agent Cust No \u2013 The agent\u2019s code in our system. This will go in shiphead.foreign-agent. Email Booking Details to Address \u2013 If this user makes a change to a booking in POM Booking, then the update will be emailed to the email address specified here. Booking Approval Fields Booking Approval Cust-No \u2013 The customer code for the bookings this user should see. Typically the same as the \u201cCust No\u201d on the first tab. Allow Shipment Approval \u2013 Yes if the user should be able to move bookings from pending to approved. Allow Shipment Notes \u2013 Yes if the user should be able to add notes to a booking. Allow Delivery Location Change \u2013 Yes if the user should be able to change the delivery location on a booking. General Access tab: App Code Access If it\u2019s an ecomm user, click \u201cEcomm User\u201d. This will set everything you need. Otherwise, see if a user already exists for the customer. If so, copy permissions from them. If you can\u2019t do either of the above, then go back to the person who requested this user and ask what access they should have. Reports Determine which reports the user should be able to see on the Reports page. POM Access tab: These are PO-related pages.","title":"New Shapiro 360 Login"},{"location":"db/","text":"Database Class /progs/mxp/shap102/objects/DB.cls Examples Read the export date of a shipment define variable dbc as class DB. define variable query-string as character no-undo. define variable export-date as date no-undo. /* init the DB connection */ dbc = new DB(\"test\"). /* set your query */ assign query-string = \"for each shiphead where shiphead.id = *\". /* set the ID you're looking for */ dbc:bind(\"2039337\"). /* run the query */ dbc:find(\"shiphead\", query-string). /* pull the first record */ dbc:next(). /* display the export date */ display date(dbc:get(\"shiphead\", \"export-date\")). Update the export date of a shipment define variable dbc as class DB. define variable query-string as character no-undo. define variable export-date as date no-undo. /* init the DB connection */ dbc = new DB(\"test\"). /* set your query */ assign query-string = \"for each shiphead where shiphead.id = *\"). /* set the ID you're looking for */ dbc:bind(\"2039337\"). /* run the query */ dbc:find(\"shiphead\", query-string). /* pull the first record */ dbc:next(). /* change the export date */ /* (don't worry, the real export date is actually 9/13/2017) */ dbc:assign(\"export-date\", 09/13/2017). For each /* query all 40' high cubes on 2039144 */ dbc:bind(\"2039144\"). dbc:bind(\"4400\"). dbc:find(\"shiphead,bookeq\", \"for each shiphead where shiphead.id = *, \" + \"each bookeq where \" + \"bookeq.branch = shiphead.branch and \" + \"bookeq.dept = shiphead.dept and \" + \"bookeq.id = shiphead.id and \" + \"bookeq.equip-type = *\"). /* keep pulling containers until you have them all */ do while(dbc:next()): message dbc:get(\"bookeq\", \"container-number\"). end. Create record /* now to create a record in the notes table */ dbc:create(\"notes\"). dbc:prepare(\"branch\", \"01\"). dbc:prepare(\"dept\", \"02\"). dbc:prepare(\"id\", \"2044354\"). dbc:prepare(\"seq\", 61). dbc:prepare(\"note-date\", today). dbc:prepare(\"note-time\", time). dbc:prepare(\"note-type\", \"P\"). dbc:prepare(\"note-text\", \".\"). dbc:flush(). Delete record /* let's delete the note before anyone sees it */ assign query-string = \"for each notes where \" + \"notes.branch = * and \" + \"notes.dept = * and \" + \"notes.id = * and \" + \"notes.seq = * and \" + \"notes.note-text = *\". dbc:bind(\"01\"). dbc:bind(\"02\"). dbc:bind(\"2044354\"). dbc:bind(61). dbc:bind(\".\"). dbc:find(\"notes\", query-string). dbc:next(). dbc:delete(). Methods To use the DB class, you will: Create and initialize the DB class into a variable bind() query variables Run find() to run the query itself Call next() to get the first result of your query get() any variables you need define variable dbc as class DB. dbc = new DB(\"\"). dbc:bind(\"2039337\"). dbc:find(\"shiphead\", \"for each shiphead where shiphead.id = *\"). dbc:next(). display date(dbc:get(\"shiphead\", \"export-date\")). There is also support for creating, updating, and deleting records. Bind Input a variable to insert into your query If you concatenate variables on your query string, you are vulnerable to code injection. To avoid injection, you must use bind(). To use the bind function, just pass in the variable you want. The datatype does not matter. (The bind() method is overridden so that you can pass in any datatype.) Bind will fill asterisks (*) sequentially. For example, say this is your query: for each shiphead where shiphead.cust-no = * and shiphead.create-date = * And you run two binds like this: dbc:bind(\u201cFIVEBMER\u201d). dbc:bind(01/01/2018). Then the query will resolve to this: for each shiphead where shiphead.cust-no = \u201cFIVEBMER\u201d and shiphead.create-date = 01/01/2018 Because you ran bind() on FIVEBMER first and 01/01/2018 second. Find Input A comma-delimited list of table names in your query The query string For example, \"for each shiphead where shiphead.id = *\" Output Success - yes/no This opens your query. After calling this, call next() to get the results. It reads the tt-bind temp-table to insert binds where you had asterisks in your query. Then, once the query is fully formed, it's stored in tt-buffer. Next next() Output Success - yes/no This will grab the next record of your query. If next() returns yes, then you can call a getter method to retrieve data about the record. If next() returns no, there were no more results from your query. Getters get, get-logical, get-date, get-integer, get-decimal, get-rowid, get-recid Use this to get the contents of a field in your queried record. If your query has one table, you can pass in a lone input parameter: the field name. dbc:get('export-date'). If your query has multiple tables, you will need to specify the table, then the field. dbc:get('shiphead', 'export-date'). If the field has an extent, you will need to pass that in along with the table and field name. dbc:get('cust-po-lines', 'custom-attr', 2). The get-* functions cast the return value to a certain datatype. get-date, for instance, will cast the output value to a date. get-curr-rowid Input Table name Output Row ID Given a table name, this will output the row ID for the record in your query. This: dbc:bind(\"2039337\"). dbc:find(\"shiphead\", \"for each shiphead where shiphead.id = *\"). if dbc:next() then do: assign shiphead-rowid = dbc:get-curr-rowid(\"shiphead\"). end. Will return the same row ID as this: find first shiphead where shiphead.id = \"2039337\" no-lock no-error. if available shiphead then do: assign shiphead-rowid = rowid(shiphead). end. Prepare prepare() Input table name field name new field value to assign field extent (optional--if you do not have an extent, only pass in 3 input parameters) Output Success - yes/no prepare() will queue an update to a record. You will need to call flush() to actually make the update. In this way, you can prepare() multiple updates, then flush() them all at once. Be warned: if you prepare() an update, next() will flush() your update before moving onto the next record. Flush flush() Output Success - yes/no This assigns every update that you have prepare()d. There are two flush methods: flush() and wait-and-flush(). wait-and-flush() will attempt to flush multiple times before giving up. If the update cannot be made, instead of hanging the program, wait-and-flush() will return no, and the program execution will continue. Before calling wait-and-flush(), please call set-wait(). Set Wait set-wait() Input email address - who to email if wait-and-flush() fails timeout 1 - the length of time that wait-and-flush() should try to make an update before sending an email timeout 2 - the length of time that wait-and-flush() should try to make an update before giving up Create create() Input table name Output Success - yes/no Instead of querying to find a record, this function puts a brand new record in scope. You will need to call flush() to commit the record to the database. Before then, please prepare() any relevant values to prevent a unique key conflict. Delete delete() Input table name (optional; if you only have one table in your query, you do not need to pass in an input parameter) Output Success - yes/no This deletes the record that is currently in scope. Assign assign() Input table name field name new field value to assign field extent (optional--if you do not have an extent, only pass in 3 input parameters) Output Success - yes/no This will prepare() an update and flush() it immediately. Helpers cleanup() No parameters If you run a large query, use this after you have pulled all the results you want. This will free up memory associated with the query. clean-str() Input: a string to clean Output: the string, cleaned This makes strings database-safe. It will prevent code injections. get-error() Input: N/A Output: an error string If a method returns false, you can run this to get a message describing what went wrong. get-curr-rowid() Input: the table name Output: the row ID This returns the row ID of the record you currently hold.","title":"Database Class"},{"location":"db/#database-class","text":"/progs/mxp/shap102/objects/DB.cls","title":"Database Class"},{"location":"db/#examples","text":"","title":"Examples"},{"location":"db/#read-the-export-date-of-a-shipment","text":"define variable dbc as class DB. define variable query-string as character no-undo. define variable export-date as date no-undo. /* init the DB connection */ dbc = new DB(\"test\"). /* set your query */ assign query-string = \"for each shiphead where shiphead.id = *\". /* set the ID you're looking for */ dbc:bind(\"2039337\"). /* run the query */ dbc:find(\"shiphead\", query-string). /* pull the first record */ dbc:next(). /* display the export date */ display date(dbc:get(\"shiphead\", \"export-date\")).","title":"Read the export date of a shipment"},{"location":"db/#update-the-export-date-of-a-shipment","text":"define variable dbc as class DB. define variable query-string as character no-undo. define variable export-date as date no-undo. /* init the DB connection */ dbc = new DB(\"test\"). /* set your query */ assign query-string = \"for each shiphead where shiphead.id = *\"). /* set the ID you're looking for */ dbc:bind(\"2039337\"). /* run the query */ dbc:find(\"shiphead\", query-string). /* pull the first record */ dbc:next(). /* change the export date */ /* (don't worry, the real export date is actually 9/13/2017) */ dbc:assign(\"export-date\", 09/13/2017).","title":"Update the export date of a shipment"},{"location":"db/#for-each","text":"/* query all 40' high cubes on 2039144 */ dbc:bind(\"2039144\"). dbc:bind(\"4400\"). dbc:find(\"shiphead,bookeq\", \"for each shiphead where shiphead.id = *, \" + \"each bookeq where \" + \"bookeq.branch = shiphead.branch and \" + \"bookeq.dept = shiphead.dept and \" + \"bookeq.id = shiphead.id and \" + \"bookeq.equip-type = *\"). /* keep pulling containers until you have them all */ do while(dbc:next()): message dbc:get(\"bookeq\", \"container-number\"). end.","title":"For each"},{"location":"db/#create-record","text":"/* now to create a record in the notes table */ dbc:create(\"notes\"). dbc:prepare(\"branch\", \"01\"). dbc:prepare(\"dept\", \"02\"). dbc:prepare(\"id\", \"2044354\"). dbc:prepare(\"seq\", 61). dbc:prepare(\"note-date\", today). dbc:prepare(\"note-time\", time). dbc:prepare(\"note-type\", \"P\"). dbc:prepare(\"note-text\", \".\"). dbc:flush().","title":"Create record"},{"location":"db/#delete-record","text":"/* let's delete the note before anyone sees it */ assign query-string = \"for each notes where \" + \"notes.branch = * and \" + \"notes.dept = * and \" + \"notes.id = * and \" + \"notes.seq = * and \" + \"notes.note-text = *\". dbc:bind(\"01\"). dbc:bind(\"02\"). dbc:bind(\"2044354\"). dbc:bind(61). dbc:bind(\".\"). dbc:find(\"notes\", query-string). dbc:next(). dbc:delete().","title":"Delete record"},{"location":"db/#methods","text":"To use the DB class, you will: Create and initialize the DB class into a variable bind() query variables Run find() to run the query itself Call next() to get the first result of your query get() any variables you need define variable dbc as class DB. dbc = new DB(\"\"). dbc:bind(\"2039337\"). dbc:find(\"shiphead\", \"for each shiphead where shiphead.id = *\"). dbc:next(). display date(dbc:get(\"shiphead\", \"export-date\")). There is also support for creating, updating, and deleting records.","title":"Methods"},{"location":"db/#bind","text":"Input a variable to insert into your query If you concatenate variables on your query string, you are vulnerable to code injection. To avoid injection, you must use bind(). To use the bind function, just pass in the variable you want. The datatype does not matter. (The bind() method is overridden so that you can pass in any datatype.) Bind will fill asterisks (*) sequentially. For example, say this is your query: for each shiphead where shiphead.cust-no = * and shiphead.create-date = * And you run two binds like this: dbc:bind(\u201cFIVEBMER\u201d). dbc:bind(01/01/2018). Then the query will resolve to this: for each shiphead where shiphead.cust-no = \u201cFIVEBMER\u201d and shiphead.create-date = 01/01/2018 Because you ran bind() on FIVEBMER first and 01/01/2018 second.","title":"Bind"},{"location":"db/#find","text":"Input A comma-delimited list of table names in your query The query string For example, \"for each shiphead where shiphead.id = *\" Output Success - yes/no This opens your query. After calling this, call next() to get the results. It reads the tt-bind temp-table to insert binds where you had asterisks in your query. Then, once the query is fully formed, it's stored in tt-buffer.","title":"Find"},{"location":"db/#next","text":"next() Output Success - yes/no This will grab the next record of your query. If next() returns yes, then you can call a getter method to retrieve data about the record. If next() returns no, there were no more results from your query.","title":"Next"},{"location":"db/#getters","text":"","title":"Getters"},{"location":"db/#get-get-logical-get-date-get-integer-get-decimal-get-rowid-get-recid","text":"Use this to get the contents of a field in your queried record. If your query has one table, you can pass in a lone input parameter: the field name. dbc:get('export-date'). If your query has multiple tables, you will need to specify the table, then the field. dbc:get('shiphead', 'export-date'). If the field has an extent, you will need to pass that in along with the table and field name. dbc:get('cust-po-lines', 'custom-attr', 2). The get-* functions cast the return value to a certain datatype. get-date, for instance, will cast the output value to a date.","title":"get, get-logical, get-date, get-integer, get-decimal, get-rowid, get-recid"},{"location":"db/#get-curr-rowid","text":"Input Table name Output Row ID Given a table name, this will output the row ID for the record in your query. This: dbc:bind(\"2039337\"). dbc:find(\"shiphead\", \"for each shiphead where shiphead.id = *\"). if dbc:next() then do: assign shiphead-rowid = dbc:get-curr-rowid(\"shiphead\"). end. Will return the same row ID as this: find first shiphead where shiphead.id = \"2039337\" no-lock no-error. if available shiphead then do: assign shiphead-rowid = rowid(shiphead). end.","title":"get-curr-rowid"},{"location":"db/#prepare","text":"prepare() Input table name field name new field value to assign field extent (optional--if you do not have an extent, only pass in 3 input parameters) Output Success - yes/no prepare() will queue an update to a record. You will need to call flush() to actually make the update. In this way, you can prepare() multiple updates, then flush() them all at once. Be warned: if you prepare() an update, next() will flush() your update before moving onto the next record.","title":"Prepare"},{"location":"db/#flush","text":"flush() Output Success - yes/no This assigns every update that you have prepare()d. There are two flush methods: flush() and wait-and-flush(). wait-and-flush() will attempt to flush multiple times before giving up. If the update cannot be made, instead of hanging the program, wait-and-flush() will return no, and the program execution will continue. Before calling wait-and-flush(), please call set-wait().","title":"Flush"},{"location":"db/#set-wait","text":"set-wait() Input email address - who to email if wait-and-flush() fails timeout 1 - the length of time that wait-and-flush() should try to make an update before sending an email timeout 2 - the length of time that wait-and-flush() should try to make an update before giving up","title":"Set Wait"},{"location":"db/#create","text":"create() Input table name Output Success - yes/no Instead of querying to find a record, this function puts a brand new record in scope. You will need to call flush() to commit the record to the database. Before then, please prepare() any relevant values to prevent a unique key conflict.","title":"Create"},{"location":"db/#delete","text":"delete() Input table name (optional; if you only have one table in your query, you do not need to pass in an input parameter) Output Success - yes/no This deletes the record that is currently in scope.","title":"Delete"},{"location":"db/#assign","text":"assign() Input table name field name new field value to assign field extent (optional--if you do not have an extent, only pass in 3 input parameters) Output Success - yes/no This will prepare() an update and flush() it immediately.","title":"Assign"},{"location":"db/#helpers","text":"cleanup() No parameters If you run a large query, use this after you have pulled all the results you want. This will free up memory associated with the query. clean-str() Input: a string to clean Output: the string, cleaned This makes strings database-safe. It will prevent code injections. get-error() Input: N/A Output: an error string If a method returns false, you can run this to get a message describing what went wrong. get-curr-rowid() Input: the table name Output: the row ID This returns the row ID of the record you currently hold.","title":"Helpers"},{"location":"mr-asn/","text":"M+R ASN Load imp/mrship-v2.i imp/mrship-v2-ocean.p imp/mrship-v2-air.p Background Scheduled M+R contacts Shapiro through FTP. They place files into /usr5/ftproot/mrspedag/inbox. There is a flow of programs from one to the next: The demon1 crontab runs /usr/bin/hourlyedi. That runs /usr/bin/runmrfwd2. That runs /usr/bin/mxpmrfwd.pf. That runs /progs/mxp/mxp81e/shapsrc/imp/mrfwd.p. imp/mrfwd.p examines each XML file in /usr5/ftproot/mrspedag/inbox and labels the EDI type. If the EDI type is ocean ASN, it gets sent to imp/mrship-v2-ocean.p. If the EDI type is air ASN, it gets sent to imp/mrship-v2-air.p. The workings of imp/mrfwd.p are a matter for another document. Automation This can be run through \u201cmredidash\u201d in Automation. Click \u201cFind Shapiro File\u201d. Insert the M+R reference number. This is the agent-edi-queue.agent-id and the shiphead.agent-id. You can find it on page 2 of Import Entry. Once you have that, highlight the EDI you want to re-run, and click \u201cReprocess\u201d. If the EDI is an ASN, this will run the ASN program with force = yes. The EDI is an ASN if \u201cData File Type\u201d is \u201cSHIP\u201d. Scroll to the right in the browser to see it. Note: this means there will be no regard for whether the ASN deletes POs or other important information. It will run no matter what. Program The program writes M+R data into temp-tables named after real database tables (tt-shiphead, tt-bookeq, etc.). After everything is written, those temp-tables are buffer-copied into the database proper, like so: buffer-copy tt-shiphead into shiphead. In the below documentation: \u201cInclude\u201d refers to mrship.i. \u201cOcean\u201d refers to mrship-v2-ocean.p. \u201cAir\u201d refers to mrship-v2-air.p. Parameters These are the same between Ocean and Air. Input rowid-agent-edi-queue This is the row ID of an agent-edi-queue record. One M+R update can create a family of database tables. The agent-edi-queue record ties them all together. force This is a logical. Normally, force should be no. This means we will not run the EDI if there is a problem. (For example, we will prevent the EDI from deleting POs on a file.) If force is yes, then we will run the update heedless of the consequences. Body The overall flow of procedures: i-process-edi \u2013 Run the update and save it to the database. p-do-update \u2013 Create temp-tables from M+R data. p-update-shiphead p-update-bolhd p-update-bolparties p-default-bookeq p-update-bookeq p-update-commodity-detail i-update-boleq i-save-to-db \u2013 Save the M+R temp-tables to the database. Internal Procedures If a procedure name begins \u201ci-\u201c, it appears in the Include. If a procedure name begins \u201cp-\u201c, it appears in Ocean, Air, or both. A procedure will be in the Include if its logic is common to both ocean and air files. Otherwise, if the logic is particular, the procedure will appear in Ocean, Air, or both. The programs are split up because ocean ASNs go into mr-shipment and air ASNs go into mr-bl. The logic is similar but based off of different database tables and fields. Include i-process-edi Labels the EDI as a consolidation or a normal file. Checks if the update should be run (i-can-update). Runs the update (p-do-update). Assigns the request status in agent-edi-queue. i-can-update Skips the update if there is an NEUP problem code. If the ONBD problem code is solved, and you are going to erase certain equipment data, don\u2019t run the update. This builds an email using p-prep-equip-diff. i-add-bolparties Add a BOL party to a shipment. This creates tt-bolparties records to be buffer-copied into the database later. i-save-to-db Buffer-copy all of your temp-tables into the database. This also handles shiphead maintenance. Create problems Write notes Send emails Create the agent file cross reference (translates the agent ID to a Shapiro ID) Update Vbin Run diffs i-chk-credit-limit Send an alert based on certain conditions. This just calls external programs to do all of the work. i-save-bolhd Buffer-copies: tt-bolhd into bolhd tt-bolparties into bol-parties i-save-bookeq Buffer-copies tt-bookeq into bookeq tt-boleq into boleq tt-commodity into commodity tt-bol-commodity into bol-commodity tt-commodity-ext-desc into commodity-ext-desc tt-bol-commodity-ext-desc into bol-commodity-ext-desc tt-commodity-detail into commodity-detail tt-bol-commodity-detail into bol-commodity-detail i-update-cust-po Update POs to label them as booked. Send an alert if you see a bad I&K PO (i-store-email). i-store-note Create a tt-ship-note record. This will be written into the notes table later. The point of this is to prevent writing notes if the program encounters an error. i-write-notes Runs through all of your tt-ship-note records and creates proper notes records. i-store-email Store an email code to be sent later. A code maps to a particular type of email. i-send-emails Send each email that was stored using i-store-email. i-crt-single-prob Create a ship-problems record. i-update-boleq Create tt-bol records based on tt-bookeq and tt-commodity tables. The idea is to recreate all of your equipment information in BOL tables. i-crt-equip-diff Go through every tt-bookeq record and compare it to its matching bookeq record. The point is to list the differences between what\u2019s in bookeq now and what would be in bookeq if we ran the update. This is used by p-prep-equip-diff, which in turn is used by i-can-update. Ocean p-do-update For this EDI, find the related M+R tables. agent-edi-queue will point to mr-shipment. mr-shipment will point to mr-clp and mr-ship-po. Run the update. p-update-shiphead p-update-bolhd p-update-bolparties p-default-bookeq p-update-bookeq p-update-commodity-detail i-update-boleq i-save-to-db This will run updates differently if an EDI is for a consolidation shipment or a regular shipment. If it\u2019s a regular shipment, there will be only one mr-shipment record. If it\u2019s a consolidation, there will be one mr-shipment record per order on the consolidation. If you have customer-specific logic, please consider putting it in procedure p-do-update-customer-specific. p-update-bolparties Read BOL parties out of mr-shipment. p-update-bolhd Read BOL header information out of mr-shipment. p-cancel-consol-input If this update is a consolidation, cancel any bookings that went into this consolidation. In a consolidation, there are a series of mr-shipment records. They share one job number (job-no), and they each have a unique order number (agent-id). The order number will match the shiphead.agent-id on a shipment. Cancel that file. If an order gets taken off of a consolidation, go back and un-cancel that file. p-update-shiphead Create the tt-shiphead record for applying updates. Check if you want to skip shiphead-level updates (p-skip-shiphead). Read mr-shipment and apply updates to tt-shiphead. p-location-notes p-update-shiphead calls this to write notes and create isf-parties based on the mr-stuffing and mr-consolidator tables. p-crt-probs Create any ship-problems applicable to a shipment. Calls i-crt-single-prob. p-skip-shiphead p-update-shiphead calls this to check if you should skip shiphead-level updates. Skip shiphead-level updates if ONBD is solved. p-create-tt-shiphead Called by p-update-shiphead to initialize the tt-shiphead record for applying updates. Try to find an existing shiphead. If it doesn\u2019t exist, create it and set it up (p-shiphead-setup). p-find-shiphead Called by p-create-tt-shiphead to see if a shiphead already exists for this shipment. Look in agent-file-cross-ref. If that doesn\u2019t work, try to match on bookeq.master-bl. p-shiphead-setup Default a shiphead record from scratch. This is used if you can\u2019t find an existing shiphead for your update. Apply information based on mr-office and agent-cns-cust-ref (agent/consignee customer reference\u2014this gets you the cust-no). p-reset-bookeq Clear out equipment tables before saving updates to the database. This is called by i-save-to-db. This will set an error and prevent saving to the database if: The shipment currently has a bookeq record The tt-bookeq table is empty This means we\u2019d delete all containers and create nothing. This also applies to the commodity-detail table. p-update-bookeq Create tt-bookeq records based on mr-shipment and mr-clp. Runs p-update-commodity-detail for any POs associated with this container. p-update-commodity-detail Create tt-commodity-detail records based on mr-ship-po. If there\u2019s no container associated with mr-ship-po, see if any other mr-ship-po records match your style number. If there are some with a container number, don\u2019t run this update. If there are none with a container number, run this update. Default the PO into the first container. p-default-bookeq Create a container for each order just by default. This doesn\u2019t run if you have an mr-clp record for the order. Create tt-bookeq based on mr-shipment. p-default-commodity Called by p-default-bookeq. Create a default tt-commodity record. p-prep-equip-diff Call p-update-bookeq to create tt-bookeq records. We won\u2019t save tt-bookeq to the database. This is just to compare the EDI against what\u2019s in our database. This calls i-crt-equip-diff to find the difference between tt-bookeq and bookeq. p-do-update-customer-specific Runs customer-specific logic for files. Air p-do-update For this EDI, find the related M+R tables. agent-edi-queue will point to mr-bl. mr-bl will point to mr-bl-po. Run the update. p-update-shiphead p-update-bookeq p-update-commodity-detail i-save-to-db This will run updates differently if an EDI is for a consolidation shipment or a regular shipment. If it\u2019s a regular shipment, there will be only one mr-bl record. If it\u2019s a consolidation, there will be one mr-bl record per order on the consolidation. p-cancel-consol-input If this update is a consolidation, cancel any bookings that went into this consolidation. In a consolidation, there are a series of mr-bl records. They share one job number (job-no), and they each have a unique order number (agent-id). The order number will match the shiphead.agent-id on a shipment. Cancel that file. This does not have the un-cancel logic that Ocean does. p-update-shiphead Create the tt-shiphead record for applying updates. Check if you want to skip shiphead-level updates (p-skip-shiphead). Read mr-bl and apply updates to tt-shiphead. p-crt-probs Create any ship-problems applicable to a shipment. Calls i-crt-single-prob. p-skip-shiphead p-update-shiphead calls this to check if you should skip shiphead-level updates. Skip shiphead-level updates if ONBD is solved. p-create-tt-shiphead Called by p-update-shiphead to initialize the tt-shiphead record for applying updates. Try to find an existing shiphead. If it doesn\u2019t exist, create it and set it up (p-shiphead-setup). p-find-shiphead Called by p-create-tt-shiphead to see if a shiphead already exists for this shipment. Look in agent-file-cross-ref. This does not try to make a match on bookeq.master-bl. p-shiphead-setup Default a shiphead record from scratch. This is used if you can\u2019t find an existing shiphead for your update. Apply information based on mr-office and agent-cns-cust-ref (agent/consignee customer reference\u2014this gets you the cust-no). p-reset-bookeq Clear out equipment tables before saving updates to the database. This is called by i-save-to-db. This will set an error and prevent saving to the database if: The shipment currently has a bookeq record The tt-bookeq table is empty This means we\u2019d delete all containers and create nothing. This also applies to the commodity-detail table. p-update-commodity-detail Create tt-commodity-detail records based on mr-bl-po. There\u2019s no container logic to worry about. It just defaults POs into the first container for this order. (If it\u2019s a consolidation, then there could be POs in multiple containers, since each order would have its own container.) p-update-bookeq Create a tt-bookeq record for this shipment based on mr-bl. Air ASNs do not have a table for specifying containers. p-update-commodity Default a tt-commodity record based on tt-bookeq. p-addl-commodity Create additional tt-commodity records based on mr-bl-dim. p-update-commodity-ext-desc Create tt-commodity-ext-desc records based on mr-bl-desc. p-prep-equip-diff Call p-update-bookeq to create tt-bookeq records. We won\u2019t save tt-bookeq to the database. This is just to compare the EDI against what\u2019s in our database. This calls i-crt-equip-diff to find the difference between tt-bookeq and bookeq. Job Type From: M+R HKG - YINNIS SO [mailto:yinnis.so@hk.mrspedag.com] Sent: Wednesday, April 13, 2016 11:21 PM To: Matthew Kobussen matthew@shapiro.com Cc: Mike Baker mikeb@shapiro.com ; Bob Kimmel bob@shapiro.com ; 'M+R HKG - SING TSANG' sing.tsang@hk.mrspedag.com Subject: M+R Job Number Definition Hi Matt, As discussed, following is the \u201cJob Type\u201d (4th character) definition of M+R Job Number. Best Regards, Yinnis So General Manager IT Asia Service Code From: M+R HKG - Ruby Wong [mailto:ruby.wong@hk.mrspedag.com] Sent: Friday, July 28, 2017 05:03 To: 'M+R SHA - Sophia Jun' sophia.jun@cn.mrspedag.com ; Hasmiou Idrissou Hasmiou@shapiro.com ; Gregory Livingston gregory@shapiro.com Cc: transcustsurf transcustsurf@shapiro.com ; 'judy.yang.sha@cn.mrspedag.com' judy.yang@cn.mrspedag.com ; 'M+R SHA - Ada Huang' ada.huang@cn.mrspedag.com ; 'M+R HKG - SING TSANG' sing.tsang@hk.mrspedag.com ; ruby.wong@hk.mrspedag.com Subject: RE: SSH-FUJMB-182 (Shapiro 2029999)and SSH-FUJMB-181 (Shapiro 2029998).............FW: 2029998 Hi Greg, Please advise if 360 started to use the \u201cServiceCode\u201d to identify consolidated shipments. For SSH-FUJMB-181, MBL# of the 2 S/O is the same and the \u201cService Code\u201d of this job is \u201c3\u201d which indicates that this is a CONSOL booking. Attached 131457048271636250_SHW1G07438.xml for your reference. Thank you. Best Regards, Ruby Wong Logistics Department M & R Forwarding (HK) Ltd. Rm 3501, 35/F, Manhattan Place, 23 Wang Tai Road, Kowloon Bay, Hong Kong SAR. Dir: +852 3122 0806 Tel: +852 2591 0677 Fax: +852 2893 0817 Email: ruby.wong@hk.mrspedag.com Web: www.mrspedag.com","title":"M+R ASN Load"},{"location":"mr-asn/#mr-asn-load","text":"imp/mrship-v2.i imp/mrship-v2-ocean.p imp/mrship-v2-air.p","title":"M+R ASN Load"},{"location":"mr-asn/#background","text":"","title":"Background"},{"location":"mr-asn/#scheduled","text":"M+R contacts Shapiro through FTP. They place files into /usr5/ftproot/mrspedag/inbox. There is a flow of programs from one to the next: The demon1 crontab runs /usr/bin/hourlyedi. That runs /usr/bin/runmrfwd2. That runs /usr/bin/mxpmrfwd.pf. That runs /progs/mxp/mxp81e/shapsrc/imp/mrfwd.p. imp/mrfwd.p examines each XML file in /usr5/ftproot/mrspedag/inbox and labels the EDI type. If the EDI type is ocean ASN, it gets sent to imp/mrship-v2-ocean.p. If the EDI type is air ASN, it gets sent to imp/mrship-v2-air.p. The workings of imp/mrfwd.p are a matter for another document.","title":"Scheduled"},{"location":"mr-asn/#automation","text":"This can be run through \u201cmredidash\u201d in Automation. Click \u201cFind Shapiro File\u201d. Insert the M+R reference number. This is the agent-edi-queue.agent-id and the shiphead.agent-id. You can find it on page 2 of Import Entry. Once you have that, highlight the EDI you want to re-run, and click \u201cReprocess\u201d. If the EDI is an ASN, this will run the ASN program with force = yes. The EDI is an ASN if \u201cData File Type\u201d is \u201cSHIP\u201d. Scroll to the right in the browser to see it. Note: this means there will be no regard for whether the ASN deletes POs or other important information. It will run no matter what.","title":"Automation"},{"location":"mr-asn/#program","text":"The program writes M+R data into temp-tables named after real database tables (tt-shiphead, tt-bookeq, etc.). After everything is written, those temp-tables are buffer-copied into the database proper, like so: buffer-copy tt-shiphead into shiphead. In the below documentation: \u201cInclude\u201d refers to mrship.i. \u201cOcean\u201d refers to mrship-v2-ocean.p. \u201cAir\u201d refers to mrship-v2-air.p.","title":"Program"},{"location":"mr-asn/#parameters","text":"These are the same between Ocean and Air. Input rowid-agent-edi-queue This is the row ID of an agent-edi-queue record. One M+R update can create a family of database tables. The agent-edi-queue record ties them all together. force This is a logical. Normally, force should be no. This means we will not run the EDI if there is a problem. (For example, we will prevent the EDI from deleting POs on a file.) If force is yes, then we will run the update heedless of the consequences.","title":"Parameters"},{"location":"mr-asn/#body","text":"The overall flow of procedures: i-process-edi \u2013 Run the update and save it to the database. p-do-update \u2013 Create temp-tables from M+R data. p-update-shiphead p-update-bolhd p-update-bolparties p-default-bookeq p-update-bookeq p-update-commodity-detail i-update-boleq i-save-to-db \u2013 Save the M+R temp-tables to the database.","title":"Body"},{"location":"mr-asn/#internal-procedures","text":"If a procedure name begins \u201ci-\u201c, it appears in the Include. If a procedure name begins \u201cp-\u201c, it appears in Ocean, Air, or both. A procedure will be in the Include if its logic is common to both ocean and air files. Otherwise, if the logic is particular, the procedure will appear in Ocean, Air, or both. The programs are split up because ocean ASNs go into mr-shipment and air ASNs go into mr-bl. The logic is similar but based off of different database tables and fields.","title":"Internal Procedures"},{"location":"mr-asn/#include","text":"i-process-edi Labels the EDI as a consolidation or a normal file. Checks if the update should be run (i-can-update). Runs the update (p-do-update). Assigns the request status in agent-edi-queue. i-can-update Skips the update if there is an NEUP problem code. If the ONBD problem code is solved, and you are going to erase certain equipment data, don\u2019t run the update. This builds an email using p-prep-equip-diff. i-add-bolparties Add a BOL party to a shipment. This creates tt-bolparties records to be buffer-copied into the database later. i-save-to-db Buffer-copy all of your temp-tables into the database. This also handles shiphead maintenance. Create problems Write notes Send emails Create the agent file cross reference (translates the agent ID to a Shapiro ID) Update Vbin Run diffs i-chk-credit-limit Send an alert based on certain conditions. This just calls external programs to do all of the work. i-save-bolhd Buffer-copies: tt-bolhd into bolhd tt-bolparties into bol-parties i-save-bookeq Buffer-copies tt-bookeq into bookeq tt-boleq into boleq tt-commodity into commodity tt-bol-commodity into bol-commodity tt-commodity-ext-desc into commodity-ext-desc tt-bol-commodity-ext-desc into bol-commodity-ext-desc tt-commodity-detail into commodity-detail tt-bol-commodity-detail into bol-commodity-detail i-update-cust-po Update POs to label them as booked. Send an alert if you see a bad I&K PO (i-store-email). i-store-note Create a tt-ship-note record. This will be written into the notes table later. The point of this is to prevent writing notes if the program encounters an error. i-write-notes Runs through all of your tt-ship-note records and creates proper notes records. i-store-email Store an email code to be sent later. A code maps to a particular type of email. i-send-emails Send each email that was stored using i-store-email. i-crt-single-prob Create a ship-problems record. i-update-boleq Create tt-bol records based on tt-bookeq and tt-commodity tables. The idea is to recreate all of your equipment information in BOL tables. i-crt-equip-diff Go through every tt-bookeq record and compare it to its matching bookeq record. The point is to list the differences between what\u2019s in bookeq now and what would be in bookeq if we ran the update. This is used by p-prep-equip-diff, which in turn is used by i-can-update.","title":"Include"},{"location":"mr-asn/#ocean","text":"p-do-update For this EDI, find the related M+R tables. agent-edi-queue will point to mr-shipment. mr-shipment will point to mr-clp and mr-ship-po. Run the update. p-update-shiphead p-update-bolhd p-update-bolparties p-default-bookeq p-update-bookeq p-update-commodity-detail i-update-boleq i-save-to-db This will run updates differently if an EDI is for a consolidation shipment or a regular shipment. If it\u2019s a regular shipment, there will be only one mr-shipment record. If it\u2019s a consolidation, there will be one mr-shipment record per order on the consolidation. If you have customer-specific logic, please consider putting it in procedure p-do-update-customer-specific. p-update-bolparties Read BOL parties out of mr-shipment. p-update-bolhd Read BOL header information out of mr-shipment. p-cancel-consol-input If this update is a consolidation, cancel any bookings that went into this consolidation. In a consolidation, there are a series of mr-shipment records. They share one job number (job-no), and they each have a unique order number (agent-id). The order number will match the shiphead.agent-id on a shipment. Cancel that file. If an order gets taken off of a consolidation, go back and un-cancel that file. p-update-shiphead Create the tt-shiphead record for applying updates. Check if you want to skip shiphead-level updates (p-skip-shiphead). Read mr-shipment and apply updates to tt-shiphead. p-location-notes p-update-shiphead calls this to write notes and create isf-parties based on the mr-stuffing and mr-consolidator tables. p-crt-probs Create any ship-problems applicable to a shipment. Calls i-crt-single-prob. p-skip-shiphead p-update-shiphead calls this to check if you should skip shiphead-level updates. Skip shiphead-level updates if ONBD is solved. p-create-tt-shiphead Called by p-update-shiphead to initialize the tt-shiphead record for applying updates. Try to find an existing shiphead. If it doesn\u2019t exist, create it and set it up (p-shiphead-setup). p-find-shiphead Called by p-create-tt-shiphead to see if a shiphead already exists for this shipment. Look in agent-file-cross-ref. If that doesn\u2019t work, try to match on bookeq.master-bl. p-shiphead-setup Default a shiphead record from scratch. This is used if you can\u2019t find an existing shiphead for your update. Apply information based on mr-office and agent-cns-cust-ref (agent/consignee customer reference\u2014this gets you the cust-no). p-reset-bookeq Clear out equipment tables before saving updates to the database. This is called by i-save-to-db. This will set an error and prevent saving to the database if: The shipment currently has a bookeq record The tt-bookeq table is empty This means we\u2019d delete all containers and create nothing. This also applies to the commodity-detail table. p-update-bookeq Create tt-bookeq records based on mr-shipment and mr-clp. Runs p-update-commodity-detail for any POs associated with this container. p-update-commodity-detail Create tt-commodity-detail records based on mr-ship-po. If there\u2019s no container associated with mr-ship-po, see if any other mr-ship-po records match your style number. If there are some with a container number, don\u2019t run this update. If there are none with a container number, run this update. Default the PO into the first container. p-default-bookeq Create a container for each order just by default. This doesn\u2019t run if you have an mr-clp record for the order. Create tt-bookeq based on mr-shipment. p-default-commodity Called by p-default-bookeq. Create a default tt-commodity record. p-prep-equip-diff Call p-update-bookeq to create tt-bookeq records. We won\u2019t save tt-bookeq to the database. This is just to compare the EDI against what\u2019s in our database. This calls i-crt-equip-diff to find the difference between tt-bookeq and bookeq. p-do-update-customer-specific Runs customer-specific logic for files.","title":"Ocean"},{"location":"mr-asn/#air","text":"p-do-update For this EDI, find the related M+R tables. agent-edi-queue will point to mr-bl. mr-bl will point to mr-bl-po. Run the update. p-update-shiphead p-update-bookeq p-update-commodity-detail i-save-to-db This will run updates differently if an EDI is for a consolidation shipment or a regular shipment. If it\u2019s a regular shipment, there will be only one mr-bl record. If it\u2019s a consolidation, there will be one mr-bl record per order on the consolidation. p-cancel-consol-input If this update is a consolidation, cancel any bookings that went into this consolidation. In a consolidation, there are a series of mr-bl records. They share one job number (job-no), and they each have a unique order number (agent-id). The order number will match the shiphead.agent-id on a shipment. Cancel that file. This does not have the un-cancel logic that Ocean does. p-update-shiphead Create the tt-shiphead record for applying updates. Check if you want to skip shiphead-level updates (p-skip-shiphead). Read mr-bl and apply updates to tt-shiphead. p-crt-probs Create any ship-problems applicable to a shipment. Calls i-crt-single-prob. p-skip-shiphead p-update-shiphead calls this to check if you should skip shiphead-level updates. Skip shiphead-level updates if ONBD is solved. p-create-tt-shiphead Called by p-update-shiphead to initialize the tt-shiphead record for applying updates. Try to find an existing shiphead. If it doesn\u2019t exist, create it and set it up (p-shiphead-setup). p-find-shiphead Called by p-create-tt-shiphead to see if a shiphead already exists for this shipment. Look in agent-file-cross-ref. This does not try to make a match on bookeq.master-bl. p-shiphead-setup Default a shiphead record from scratch. This is used if you can\u2019t find an existing shiphead for your update. Apply information based on mr-office and agent-cns-cust-ref (agent/consignee customer reference\u2014this gets you the cust-no). p-reset-bookeq Clear out equipment tables before saving updates to the database. This is called by i-save-to-db. This will set an error and prevent saving to the database if: The shipment currently has a bookeq record The tt-bookeq table is empty This means we\u2019d delete all containers and create nothing. This also applies to the commodity-detail table. p-update-commodity-detail Create tt-commodity-detail records based on mr-bl-po. There\u2019s no container logic to worry about. It just defaults POs into the first container for this order. (If it\u2019s a consolidation, then there could be POs in multiple containers, since each order would have its own container.) p-update-bookeq Create a tt-bookeq record for this shipment based on mr-bl. Air ASNs do not have a table for specifying containers. p-update-commodity Default a tt-commodity record based on tt-bookeq. p-addl-commodity Create additional tt-commodity records based on mr-bl-dim. p-update-commodity-ext-desc Create tt-commodity-ext-desc records based on mr-bl-desc. p-prep-equip-diff Call p-update-bookeq to create tt-bookeq records. We won\u2019t save tt-bookeq to the database. This is just to compare the EDI against what\u2019s in our database. This calls i-crt-equip-diff to find the difference between tt-bookeq and bookeq.","title":"Air"},{"location":"mr-asn/#job-type","text":"From: M+R HKG - YINNIS SO [mailto:yinnis.so@hk.mrspedag.com] Sent: Wednesday, April 13, 2016 11:21 PM To: Matthew Kobussen matthew@shapiro.com Cc: Mike Baker mikeb@shapiro.com ; Bob Kimmel bob@shapiro.com ; 'M+R HKG - SING TSANG' sing.tsang@hk.mrspedag.com Subject: M+R Job Number Definition Hi Matt, As discussed, following is the \u201cJob Type\u201d (4th character) definition of M+R Job Number. Best Regards, Yinnis So General Manager IT Asia Service Code From: M+R HKG - Ruby Wong [mailto:ruby.wong@hk.mrspedag.com] Sent: Friday, July 28, 2017 05:03 To: 'M+R SHA - Sophia Jun' sophia.jun@cn.mrspedag.com ; Hasmiou Idrissou Hasmiou@shapiro.com ; Gregory Livingston gregory@shapiro.com Cc: transcustsurf transcustsurf@shapiro.com ; 'judy.yang.sha@cn.mrspedag.com' judy.yang@cn.mrspedag.com ; 'M+R SHA - Ada Huang' ada.huang@cn.mrspedag.com ; 'M+R HKG - SING TSANG' sing.tsang@hk.mrspedag.com ; ruby.wong@hk.mrspedag.com Subject: RE: SSH-FUJMB-182 (Shapiro 2029999)and SSH-FUJMB-181 (Shapiro 2029998).............FW: 2029998 Hi Greg, Please advise if 360 started to use the \u201cServiceCode\u201d to identify consolidated shipments. For SSH-FUJMB-181, MBL# of the 2 S/O is the same and the \u201cService Code\u201d of this job is \u201c3\u201d which indicates that this is a CONSOL booking. Attached 131457048271636250_SHW1G07438.xml for your reference. Thank you. Best Regards, Ruby Wong Logistics Department M & R Forwarding (HK) Ltd. Rm 3501, 35/F, Manhattan Place, 23 Wang Tai Road, Kowloon Bay, Hong Kong SAR. Dir: +852 3122 0806 Tel: +852 2591 0677 Fax: +852 2893 0817 Email: ruby.wong@hk.mrspedag.com Web: www.mrspedag.com","title":"Job Type"},{"location":"regenwsdl/","text":"Regen WSDL Make sure that you compile any programs that you want added to the WSDL Login to Shap-exp-360pro for Production or Shap-exp-360dev for Development. Launch WinSCP Move over the .r files, of the programs you complied in Step 1 (See below for the directory names) Launch the proxy generator Hit the folder icon and navigate to C:\\shapiro360 and chose ShapiroProd or ShapiroDev The hit Procedure->Add->Non-Persistent Hit save and then hit to generate the WSDL Next go back to WinSCP and navigate to the below folders Drag ShapiroProd.xpxg from the left panel to the right panel. If you get and error the delete the file on the right side and then drag over the file. Open up a rashi6 session and cd to /home/allegro/bin and run either xpxgdev-build.sh or xpxgprod-build.sh You should receive a message that this run was successful. Next go to http://192.168.66.102:9090 Go to the Web Services Adapter section and select either shap-exp-rashi6.wsadevShapiro360 or shap-exp-rashi6.wsaprodShapiro360 Got t Status Enablement and disable the Web Service Adaptor. Then go back to the previous screen and hit Update. Type /progs/mxp/shap102/objects/ShapiroProd.wsm or /progs/mxp/shap102test/objects/ShapiroDev.wsm and hit submit. Click through the next page and you should receive a message that it was updated. Lastly right click and open Configure Tomcat as administrator Click stop, wait about 10 seconds and then start tomcat.","title":"Regen WSDL"},{"location":"regenwsdl/#regen-wsdl","text":"Make sure that you compile any programs that you want added to the WSDL Login to Shap-exp-360pro for Production or Shap-exp-360dev for Development. Launch WinSCP Move over the .r files, of the programs you complied in Step 1 (See below for the directory names) Launch the proxy generator Hit the folder icon and navigate to C:\\shapiro360 and chose ShapiroProd or ShapiroDev The hit Procedure->Add->Non-Persistent Hit save and then hit to generate the WSDL Next go back to WinSCP and navigate to the below folders Drag ShapiroProd.xpxg from the left panel to the right panel. If you get and error the delete the file on the right side and then drag over the file. Open up a rashi6 session and cd to /home/allegro/bin and run either xpxgdev-build.sh or xpxgprod-build.sh You should receive a message that this run was successful. Next go to http://192.168.66.102:9090 Go to the Web Services Adapter section and select either shap-exp-rashi6.wsadevShapiro360 or shap-exp-rashi6.wsaprodShapiro360 Got t Status Enablement and disable the Web Service Adaptor. Then go back to the previous screen and hit Update. Type /progs/mxp/shap102/objects/ShapiroProd.wsm or /progs/mxp/shap102test/objects/ShapiroDev.wsm and hit submit. Click through the next page and you should receive a message that it was updated. Lastly right click and open Configure Tomcat as administrator Click stop, wait about 10 seconds and then start tomcat.","title":"Regen WSDL"},{"location":"report/","text":"Tables The \u201cname\u201d field is the ID which links all tables. rpt-hdr This is the header; a report will not be acknowledged unless it has an rpt-hdr record. name The program-friendly name for a report. Please don\u2019t use spaces or goofy characters. Try to make it alphanumerics, underscores, and dashes only. It also goes in the filename of the spreadsheet, so don\u2019t put anything uncouth. run-time The time of day that this should run. For example: \u201c7:30AM\u201d, \u201c8PM\u201d. run-day The day(s) that this should run. If you want days of the week, specify them like this: \u201cMonday,Tuesday,Wednesday,Thursday,Friday\u201d If you want days of the month, specify them like this: \u201c1,15\u201d calc-prog The name of an external program to reference. Its behavior is dependent on rpt-hdr.use-remote. use-remote If this is yes, then it will run as an external report. This ignores rpt-qry and rpt-det tables. It will call rpt-hdr.calc-prog like this: run value(rpt-hdr.calc-prog) ( input rpt-hdr.name, input full-dir + \"/\", input login ) no-error. If this is no, then it will run as an internal report. This means looking up rpt-qry and rpt-det tables to generate a report. If rpt-hdr.calc-prog is set, it will be run as a filter. rpt-version A number representing the version of the report. party The party associated with this report. Set this to customer.cust-no or the name of a department (\u201cCompliance\u201d, \u201cGLG\u201d, etc.). rpt-desc The human-readable name for this report. shap360-name If you leave this blank, no one in Shapiro 360 will see it. If you set this, it will be visible in Shapiro 360. The value of rpt-hdr.party must be in web-login.us-stats-cust-no-allowed. shap360-users If this report is visible in Shapiro 360, you can lock it down to certain 360 users. If you add names here, it will only be visible to those users. If you leave this blank, it will be visible to all users. rpt-xls This table defines the email which sends this report. name * Should match rpt-hdr.name. col-name * For use in internal reports. Filter on a certain column name when generating the spreadsheet. cell-value * For use in internal reports. Filter on a certain column value when generating the spreadsheet. from-addr * For the email, the address that should send it. Commonly \u201cshapiroautomation@shapiro.com\u201d. to-addr * The recipient email for this report. You can send to multiple emails if you separate them by commas. rpt-title * The subject line of the email. rpt-body * The body of the email. If you leave this blank, it will send a generic message. send-if-blank * If this is yes, you\u2019ll try to send the report even if you don\u2019t have one to send. rpt-inp If your report needs user input, define those input fields here. When the report is run on a schedule, it will look for fields where rpt-inp.login is \u201cSYSTEM\u201d. For an example of this, see imp/report-fb-all.p:get-date-range. name Should match rpt-hdr.name. login The login of the user. This can be \u201cSYSTEM\u201d (when run on a schedule), an Automation login (when run from Automation), or a Joomla numeric user ID (when run from Shapiro 360). inp-seq Specifies the order that this should be printed. 1 is printed first, then 2, then 3, etc. inp-name The name of the input field. Put something that will look nice when the user sees it. Instead of \u201cstart-date\u201d, put \u201cStart Date\u201d. inp-value The value that the user set for this input field. inp-desc The description of the input field. Put something that will give the user instructions. For example, if inp-name is \u201cStart Date\u201d, inp-desc could be \u201cSet the starting date for this report.\u201d inp-type The datatype of this field. This is important for displaying the report properly in Shapiro 360. Important Note The tables below this are only used for internal reports. \u201cInternal\u201d does not mean \u201cinternal to Shapiro\u201d. It means \u201cthe query is run internally inside of imp/report.p\u201d. If rpt-hdr.use-remote is yes, you can ignore the below tables. rpt-qry This defines the query for finding records to put on the report. Each rpt-qry record defines one phrase in the \u201cwhere\u201d clause. Each rpt-qry record is joined by \u201cand\u201d to create the full where clause. name Should match rpt-hdr.name. table-depth There can be multiple queries in a report. This specifies how many queries deep you are. For instance, say you want to find every bookeq for every shiphead. Shiphead will be table-depth 1. Bookeq will be table-depth 2. depth-seq Ignore this. It should always be 1. You can use it to set up \u201cor\u201d-like behavior if you super seriously need to. table-name The table to query. \u201cshiphead\u201d, \u201cbookeq\u201d, etc. field-name The field to compare against. \u201ccreate-date\u201d, \u201ccontainer-number\u201d, etc. operator The verb to use. \u201c=\u201d, \u201cbegins\u201d, etc. field-value The value to compare against. If field-type is \u201cdate\u201d, you can use an integer. For instance, if you set field-value to \u201c-5\u201d, then it will be calculated as \u201ctoday - 5\u201d. If local-var is yes, then set this to the name of a local variable. For example, \u201cbranch\u201d. field-type The datatype to compare against. This is important for doing date/numeric comparisons. local-var If this is yes, then you will compare against a local variable as defined in the rpt-var table. rpt-var When running multi-table queries, you\u2019ll need some way to link one table to another. For example, to find every bookeq for every shiphead, you\u2019ll need to compare branch, dept, and id. rpt-var stores those as local variables at runtime. name Should match rpt-hdr.name. table-depth The depth to pull this variable from. Should match rpt-qry.table-depth. depth-seq Should match rpt-qry.depth-seq. field-name The field to pull as a local variable. For example, if you\u2019re pulling from shiphead, this might be \u201cbranch\u201d. rpt-det Each rpt-det record defines a column on the report. name Should match rpt-hdr.name. field-seq The order of this column on the report. table-name The name of the table to pull this field from. For example, \u201cshiphead\u201d. field-name The name of the field to pull this data from. For example, \u201cbranch\u201d. col-name The name of the column to display on the spreadsheet. data-type The Progress datatype. excel-type The Excel datatype. This is important for formatting columns. calc-prog If you need to calculate this field, leave field-name blank. Then, set calc-prog to a program name. It will be called like this: run value(rpt-det.calc-prog) ( input rpt-hdr.name, input rpt-det.table-name, input dbc:get-curr-rowid(rpt-det.table-name), input rpt-det.col-name, input login, output rpt-cell.cell-value ) no-error. hidden Use this if you want to hide the value of this column. This is used when you want to pull different rows onto different tabs. rpt-tab If you want more than one tab on your report, create an rpt-tab record. name Should match rpt-hdr.name. col-name The name of the column to generate tabs from. This should match rpt-det.col-name. cell-value If cell-value is blank, you will create one tab for every unique value in a column. If you want to filter to one value, specify it here. Central Program Rashi 6 imp/report.p Parameters Input Report time This determines the output location of the report. If it\u2019s a time (such as \u201c8AM\u201d, \u201c10AM\u201d, or \u201c1stOfMonth\u201d), it will end up in a directory where schedtask will generate the .xls at a given time. If it\u2019s blank, it will put the report in a user-specific directory based on the user login input. An email will be sent to {user login}@shapiro.com with realtime master XLS instructions. If it\u2019s \u201cshapiro360\u201d, it will go to a special directory for Shapiro 360 report downloads. See the documentation on shapiro360/runRpt.p for more information. Report name If this is blank, it will run all reports matching a given time. For example, if \u201c8AM\u201d is your report time, and the report name is blank, it will run all reports scheduled for 8 AM. User login If the report is being run on a schedule, then this should be \u201cSYSTEM\u201d. If it\u2019s run for an internal user, this should be their Automation login. (Hopefully we can just append \u201c@shapiro.com\u201d to get their email address.) If it\u2019s run for Shapiro 360, this will be their user ID in the Joomla MySQL database. In this case, it will be an integer. Body Kicks off the reports based on your input parameters: whether it should be a scheduled job or an individual report, and whether it goes internal or out to Shapiro 360. This sets the output directory and calls wrt-rpt. It also emails the Automation user if necessary. Internal Procedures There are two kinds of reports: internal and remote. Internal reports run a database query through imp/report.p. This largely happens in get-depth. External reports run a database query in an external program. imp/report.p will call this program. External reports are easier. Internal and external wrt-rpt Writes an individual report. If you\u2019re doing a scheduled run for a certain time, you\u2019ll call wrt-rpt once for each report scheduled. Calls prep-output to set up your individual directories and sendmail scripts. If reports run inside of imp/report.p, you\u2019ll call: get-depth to traverse the rpt-qry and rpt-det tables (these tables define the database queries and assign statements which produce reports) finalize-tabs to split data into tabs if necessary If reports don\u2019t run inside of imp/report.p, you\u2019ll call remote-report to run the report. procedure get-full-dir This will generate one directory for each rpt-xls record. There will be one by default. Internal reports get-depth This one\u2019s a doozy. This builds a query using rpt-qry to find records for pulling data onto the report. One rpt-qry record represents one phrase in the query\u2019s \u201cwhere\u201d clause. rpt-det specifies which data you pull onto the report. One rpt-det record represents one column on the report. Some notes on external programs: If rpt-hdr.calc-prog is set, get-depth runs a filter function like so: run value(rpt-hdr.calc-prog) ( input rpt-hdr.name, input table-name, input table-rowid, input \"filter\", input user-login, output continue ) no-error. Use this if you want to skip certain records, but can\u2019t do so through defining rpt-qry records. (For example, if you need logic involving \u201cor\u201d.) If continue is Y, then it will put this record on the report. Otherwise, it will skip this record. If rpt-det.calc-prog is blank, then it will pull the value out of the database directly. If rpt-det.calc-prog is set, it will run that program to determine the value to put in the cell. It calls rpt-det.calc-prog like so: run value(file-info:full-pathname) ( input rpt-hdr.name, input rpt-det.table-name, input dbc:get-curr-rowid(rpt-det.table-name), input rpt-det.col-name, input login, output rpt-cell.cell-value ) no-error. prep-output Prepare directories for CSV files to go in. Write rpt-list.txt so that the master XLS knows to run this report. Append to send-script.sh so that you mail the report out (if it\u2019s scheduled). procedure open-report Create directories to house .csv and .xls files. procedure finalize-tabs Write the header line in each .csv file you\u2019ve generated. procedure open-tab Not called. procedure output-line Outputs lines in every relevant directory. procedure print-line-by-tabs Outputs lines to every relevant .csv file. print-line-to-file Read the rpt-cell table and put data in a .csv file. External reports remote-report Calls a program to generate .csv files into a directory. Calls the program like so: run value(file-info:full-pathname) ( input rpt-hdr.name, input full-dir + \"/\", input login ) no-error. Examples of Calls imp/report.p ( input \u201c7:30AM\u201d, input \u201c\u201d, input \u201cSYSTEM\u201d ). In this case, we\u2019re running all of the morning reports. imp/report.p ( input \u201cshapiro360\u201d, input \u201cfb-draft\u201d, input \u201c648\u201d ). A user on 360 is running the Five Below All PO report. The Joomla user ID is 648. imp/report.p ( input \u201crealtime\u201d, input \u201cmr-fb-pom\u201d, input \u201cgregory\u201d ). Automation user \u201cgregory\u201d is running the M+R Five Below POM report. Because the time is \u201crealtime\u201d, the realtime XLS will be emailed to \u201cgregory@shapiro.com\u201d. (It will be S:\\CORP-DAILY\\realtime\\gregory\\realtime-master.xls.) imp/report.p ( input \u201c\u201d, input \u201cfb-onwat-fb\u201d, input \u201cSYSTEM\u201d ). Someone is testing the Five Below On the Water report. They must have typed this into the Progress Editor. This run will end up in CORP-DAILY/rpt/realtime. Automation These are programs in Automation for updating and running reports. I have tried to put advice in the \u201cHelp\u201d box on the right in this program. Or rather, I have put advice in that box, and I have tried to make it good advice. Report Modifier Before starting, you must enter a report name in \u201cReport:\u201d and hit Enter. If you don\u2019t know one, hit F6 to look one up. Or, if you want to create a new report, type the name for it and hit Enter. Please make it program-friendly (alphanumerics, underscores, and dashes only). Use the \u201cRun\u201d button to run the report. It\u2019ll email you a realtime master XLS. Header Header Program If Use Remote? is yes, then Program will be called like this: run value(rpt-hdr.calc-prog) ( input rpt-hdr.name, input full-dir + \"/\", input login ) no-error. If Use Remote? is no, then Program will be used as a filter, like this: run value(rpt-hdr.calc-prog) ( input rpt-hdr.name, input table-name, input table-rowid, input \"filter\", input user-login, output continue ) no-error. Description A human-readable description for the program. Party The party for this program. You can leave it blank, set it to a customer code, or set it to a Shapiro department. Version A number representing the version of the program. Use remote? This determines the behavior of the Program field. If you set this to yes, then you can have most of the Progress programming coming out of your own program. Shapiro 360 Name If you want to display this in Shapiro 360, set this. The Party must match the customer code for a user. Shapiro 360 Users If you leave this blank, it\u2019ll display to all users matching the Party. If you set it, this report will only show to the users specified. Input Field Name The name of the field. Make it nice to look at; do \u201cStart Date\u201d, not \u201cstart-date\u201d. Default Value The default value. This will be used if the report is run on a schedule. It will also auto-populate for a user before they have set their own preferred value. Data Type The datatype for this field. It\u2019s important to specify this so you can perform accurate numeric or date comparisons. Description You can use this to give instructions to the user if you need to. Query You will only need this if \u201cUse Remote?\u201d is no. Query Every record here defines a phrase in the \u201cwhere\u201d clause. Depth If you are querying tables within tables, this specifies which query you are talking about. For instance, say you are querying every bookeq for every shiphead. shiphead would be Depth 1. bookeq would be Depth 2. Table The table to query. Field The field to compare against. Operator The verb to use: \u201c=\u201d, \u201cbegins\u201d, etc. Value The value to compare against. If Data Type is \u201cdate\u201d, then you can put an integer here to perform comparisons relative to today. For example, if you enter \u201c-5\u201d, then the comparison will be against \u201ctoday - 5\u201d. Use Local Variable? Usually this is no. But, if you want to compare against the value of a local variable, select yes. If you select yes, then \u201cvalue\u201d must be the name of a local variable (\u201cbranch\u201d, \u201cdept\u201d, etc.). Note that you must define local variables below. Data Type The datatype of the field you\u2019re comparing against. Local Variables Query Depth Which depth to pull this local variable from. If you\u2019re querying every bookeq for every shiphead: shiphead will be Depth 1. bookeq will be Depth 2. Field Name The field to pull into a local variable. In the shiphead/bookeq example: You\u2019ll want to pull \u201cbranch\u201d, \u201cdept\u201d, and \u201cid\u201d from Depth 1. Columns You will only need this if \u201cUse Remote?\u201d is no. Columns Move Up/Move Down Use these to change the order of columns on the report. The columns are printed in order from left to right. Table The table to pull this column from. Field The field name to pull this column from. Program If you need to calculate the value of this field, put in a program name. It will be called like this: run value(rpt-det.calc-prog) ( input rpt-hdr.name, input rpt-det.table-name, input dbc:get-curr-rowid(rpt-det.table-name), input rpt-det.col-name, input login, output rpt-cell.cell-value ) no-error. Column Header The title for the column to print in the spreadsheet. Hidden? Use this to hide a column. You\u2019ll want to do that if you are creating tabs. Progress Type The datatype of the variable in progress. Excel Type The format of the value in the Excel spreadsheet. This is important to set so that integers don\u2019t turn into scientific notation and so that dates don\u2019t get reformatted. Tabs Only bother with this if you want multiple tabs in your report. Column Header Set this to the Column Header from the above form. There will be one tab for each unique value in this column. For example, if you set it on the \u201cPort of Entry\u201d column, then you might have tabs like: \u201cLOS ANGELES, CA\u201d \u201cMEMPHIS, TN\u201d \u201cBALTIMORE, MD\u201d Mail To Email The email to receive this report. From Email The email to send this report. Email Subject The subject line for the email. Email Body The body text for the email. If you leave it blank, we\u2019ll send a generic message. Send if Blank? If there\u2019s no report to send, then you can prevent us from sending anything. Set this to \u201cyes\u201d if you want the mailer to run even if we don\u2019t have a report. Run time The time of day to run. Run day The day of week to run. Run month The month to run. Leave blank if it should run every month. Column Name Use this to filter reports. Probably leave it blank. Cell Value Use this to filter reports. Probably leave it blank. Report Runner Search Type in a report name, party, or description. OK Hit this to select a report from the table. You need to hit \u201cOK\u201d before you can run it. Input This table displays user input fields. The program will set up defaults for you. You can change them using the \u201cInput Value\u201d text input. Input Value Click \u201cUpdate\u201d to change an input value. Click \u201cSave\u201d to save your new input value. Run This kicks off a report. Once it\u2019s done, you will receive an email with the report. Cancel You will need to wait while a report runs. If you cannot wait, you can click \u201cCancel\u201d. If you click \u201cCancel\u201d, your report will not finish. Return Use this to exit the Report Runner. Master XLS There are two different master XLSes. One is run during a scheduled run, and another is sent to users for running in real time. Folder structure: S:\\CORP-DAILY\\rpt{time that the report runs}{report name}{spreadsheet group}{CSV files} For example: S:\\CORP-DAILY\\rpt\\10am\\fb-draft\\rpt\\allpo.csv Each spreadsheet group will have one spreadsheet. Every file in the {CSV files} folder will be put as one tab on the same spreadsheet. Scheduled S:\\CORP-DAILY\\rpt\\master.xls auto_open() Get a list of report names to run out of rpt-list.txt. This goes in a variable called ReportNames. Each report name defines a directory to look in. For each directory, examine all sub-directories. This goes in SubFolder. For each subfolder, examine all CSV files. This goes in CSVNames. Run format_cells() for each entry in CSVNames. Each CSV file is a tab on the report. For each subfolder, save an XLS. Move all CSV files into the archive directory. Delete rpt-list.txt. Quit out of the Excel macro. format_cells() If this is a special tab (marked as \u201cPIE CHART\u201d or \u201cBAR GRAPH\u201d), pass it off to pie_chart() or bar_graph() as is appropriate. If you\u2019re not a special tab, run normally. Set the borders. Examine the header cell of each column. Apply formatting (bold text, gray background). Format the column if any formatting is specified. \u201c (DATE)\u201d \u2013 format as a date \u201c (INTEGER)\u201d \u2013 format as an integer \u201c{header};NumberFormat;{format}\u201d \u2013 format the column with whatever is in {format} Example: \u201cItem Price;NumberFormat;0.0000\u201d will apply number format \u201c0.0000\u201d to the column. The header will only say \u201cItem Price\u201d afterwards. For each cell, replace the text \u201c(NEWLINE)\u201d with an actual newline. Autofit and autofilter all cells. pie_chart() First row format: PIE CHART,{name},{number of rows} \u201cPIE CHART\u201d must be there to tell the macro that it\u2019s a pie chart. {name} is the name of your pie chart. {number of rows} is the number of rows of data you\u2019re sending. Then each subsequent row must have this format: {name},{value} {name} is the name of the pie chart slice. {value} is the value associated with the pie chart slice. bar_graph() First row format: BAR GRAPH,{name},{number of rows},{special},\u201dDATA LABELS\u201d \u201cBAR GRAPH\u201d must be there to tell the macro that it\u2019s a bar graph. {name} is the name of your bar graph. {number of rows} is the number of rows of data that you\u2019re sending. {special} is optional. It lets you format the bar graph. \u201cDOUBLE BAR\u201d \u2013 show columns clustered side by side \u201c100% STACK\u201d \u2013 show columns stacked on top one another to reach 100% If you want data labels, you must put \u201cDATA LABELS\u201d in the fifth spot. This is optional. The second row must have this format: ,{name 1},{name 2} The first spot must be blank. {name 1} is the label of each bar. For example: if the bars are labelled \u201cBrazil\u201d and \u201cTurkey\u201d, then {name 1} should be \u201cCountry\u201d. {name 2} is the value for each bar. For example: if the bars show the percentage of compliance, then {name 2} should be \u201c% compliant\u201d. Then each subsequent row must have this format: {label},{value} {label} is the name of a data point. {value} is the value for a data point. Col_Letter() Find the letter corresponding to a column. HexToLongRGB() Convert a hexadecimal color code to RGB. Realtime S:\\CORP-DAILY\\rpt\\realtime-master.xls This is the same as the scheduled XLS, except it keeps the report open so the user can read it. Shapiro 360 Report Getter shapiro360/getRpt.p This program checks if you\u2019re allowed to access a report. It also passes back any input fields for the report as defined in rpt-inp. This is so that custom-report.php can display input fields for the user to fill out. For instance, say your report has a start date and an end date; this will pass back two input fields, one for each date. Parameters Input web-login.user-login rpt-hdr.name Output Error message Human-readable report name A temp table of user input fields PHP Script custom-report.php Initialization A report name is passed in through $_GET[\u2018rpt\u2019]. This report name is passed into the report getter in order to draw the page. The report getter gives you the title to display on the page. It also gives you the input fields to display. For each row in ttInput, it draws an input field. If it\u2019s an integer, the page draws a number input; if it\u2019s a date, the page draws a date picker; otherwise, the page draws a regular text input. Input Parameters This report reads the \u201crpt\u201d GET variable. This corresponds to a value in rpt-hdr.name. Functions JavaScript window.addEventListener(\u2018load\u2019) Set the page name. If you have any date inputs, initialize the date picker. run_rpt() Pack up the input fields and call the report runner. This kicks off ping_file(). ping_file() Check to see if your report has finished running. If it\u2019s done, download it. If it\u2019s not done, check again in 1 second. This uses setInterval(). The report will be at https://shapiro360.shapiro.com/vdocs/reports/{user_id} {report name} {timestamp}.xlsx. However, ping_file() checks for a .txt file. This file is created once the .xlsx is completely written. This prevents the user from downloading a half-written spreadsheet. For big reports (Five Below All PO Report), this can run for like five minutes. Report Runner shapiro360/runRpt.p Take the user\u2019s input and create rpt-inp records. Then, run the report through imp/report.p. Once that is done, call the Master PY. Parameters Input web-login.user-login Joomla user ID rpt-hdr.name timestamp input temp-table Output Error message Location of the report This isn\u2019t used by the PHP script, since the web service will time out on larger reports. That\u2019s okay, because the Progress program will continue running, even though it will never return to the PHP script. Internal Procedures crt-inp This creates rpt-inp records based on data the user passed in. This scheme allows you to save the user\u2019s input to the database so that it can be read from any program. Master PY shapiro360/openpyxl-2.5.4/report.py This uses the Python library openpyxl. Input Parameters Input folder \u2013 folder to read CSVs from Output \u2013 the filename to save an XLSX into, minus the file extension Body Read every CSV file out of the input folder one by one. Run normal_sheet(), pie_chart(), or bar_graph() as is appropriate. Save the output XLSX. Once that\u2019s done, save a TXT file to communicate that the XLSX is fully written. Functions as_text() Return the value of a cell as a string. normal_sheet() Process a CSV file as a normal sheet. This mirrors format_cells() from the Master XLS. pie_chart() Process a CSV as a pie chart. This mirrors pie_chart() from the Master XLS. bar_graph() Process a CSV as a bar graph. This mirrors bar_graph() from the Master XLS. Scheduled Runs Rashi 6: Generate .csv Files The demon1 crontab runs /usr/bin/run_report_sched.sh runs on the 15th and 45th minute of each hour. That calls /usr/bin/mxp_report.pf. That calls imp/report-sched.p. imp/report-sched.p figures the time and passes it into imp/report.p. For instance, if the time is between 7:10 AM and 7:39 AM, imp/report-sched.p will pass \"8AM\" to imp/report.p. schedtask: Generate .xls Files The Task Scheduler calls C:\\bat\\report.bat on the 15th and 45th minute of each hour. It will determine what time to run. For instance, if the time is between 7:40 AM and 8:09 AM, it will get the time \"8AM\". It will look for a directory according to this time. This will be in the format: \\myshapiro\\dfs\\corp-daily\\rpt{TIME} If the directory exists, and it has a master XLS and rpt-list.txt, then report.bat will run the master XLS. Rashi 6: Mail .xls Files The demon1 crontab runs /usr/bin/mail_report_sched.sh on the 0th and 30th minute of each hour. It will look for a mail script according to the time. The mail script will be in this location: /usr5/dailyreports/rpt/{TIME}/send-script.sh If the mail script is there, mail_report_sched.sh will run it.","title":"Tables"},{"location":"report/#tables","text":"The \u201cname\u201d field is the ID which links all tables.","title":"Tables"},{"location":"report/#rpt-hdr","text":"This is the header; a report will not be acknowledged unless it has an rpt-hdr record. name The program-friendly name for a report. Please don\u2019t use spaces or goofy characters. Try to make it alphanumerics, underscores, and dashes only. It also goes in the filename of the spreadsheet, so don\u2019t put anything uncouth. run-time The time of day that this should run. For example: \u201c7:30AM\u201d, \u201c8PM\u201d. run-day The day(s) that this should run. If you want days of the week, specify them like this: \u201cMonday,Tuesday,Wednesday,Thursday,Friday\u201d If you want days of the month, specify them like this: \u201c1,15\u201d calc-prog The name of an external program to reference. Its behavior is dependent on rpt-hdr.use-remote. use-remote If this is yes, then it will run as an external report. This ignores rpt-qry and rpt-det tables. It will call rpt-hdr.calc-prog like this: run value(rpt-hdr.calc-prog) ( input rpt-hdr.name, input full-dir + \"/\", input login ) no-error. If this is no, then it will run as an internal report. This means looking up rpt-qry and rpt-det tables to generate a report. If rpt-hdr.calc-prog is set, it will be run as a filter. rpt-version A number representing the version of the report. party The party associated with this report. Set this to customer.cust-no or the name of a department (\u201cCompliance\u201d, \u201cGLG\u201d, etc.). rpt-desc The human-readable name for this report. shap360-name If you leave this blank, no one in Shapiro 360 will see it. If you set this, it will be visible in Shapiro 360. The value of rpt-hdr.party must be in web-login.us-stats-cust-no-allowed. shap360-users If this report is visible in Shapiro 360, you can lock it down to certain 360 users. If you add names here, it will only be visible to those users. If you leave this blank, it will be visible to all users.","title":"rpt-hdr"},{"location":"report/#rpt-xls","text":"This table defines the email which sends this report. name * Should match rpt-hdr.name. col-name * For use in internal reports. Filter on a certain column name when generating the spreadsheet. cell-value * For use in internal reports. Filter on a certain column value when generating the spreadsheet. from-addr * For the email, the address that should send it. Commonly \u201cshapiroautomation@shapiro.com\u201d. to-addr * The recipient email for this report. You can send to multiple emails if you separate them by commas. rpt-title * The subject line of the email. rpt-body * The body of the email. If you leave this blank, it will send a generic message. send-if-blank * If this is yes, you\u2019ll try to send the report even if you don\u2019t have one to send.","title":"rpt-xls"},{"location":"report/#rpt-inp","text":"If your report needs user input, define those input fields here. When the report is run on a schedule, it will look for fields where rpt-inp.login is \u201cSYSTEM\u201d. For an example of this, see imp/report-fb-all.p:get-date-range. name Should match rpt-hdr.name. login The login of the user. This can be \u201cSYSTEM\u201d (when run on a schedule), an Automation login (when run from Automation), or a Joomla numeric user ID (when run from Shapiro 360). inp-seq Specifies the order that this should be printed. 1 is printed first, then 2, then 3, etc. inp-name The name of the input field. Put something that will look nice when the user sees it. Instead of \u201cstart-date\u201d, put \u201cStart Date\u201d. inp-value The value that the user set for this input field. inp-desc The description of the input field. Put something that will give the user instructions. For example, if inp-name is \u201cStart Date\u201d, inp-desc could be \u201cSet the starting date for this report.\u201d inp-type The datatype of this field. This is important for displaying the report properly in Shapiro 360.","title":"rpt-inp"},{"location":"report/#important-note","text":"The tables below this are only used for internal reports. \u201cInternal\u201d does not mean \u201cinternal to Shapiro\u201d. It means \u201cthe query is run internally inside of imp/report.p\u201d. If rpt-hdr.use-remote is yes, you can ignore the below tables.","title":"Important Note"},{"location":"report/#rpt-qry","text":"This defines the query for finding records to put on the report. Each rpt-qry record defines one phrase in the \u201cwhere\u201d clause. Each rpt-qry record is joined by \u201cand\u201d to create the full where clause. name Should match rpt-hdr.name. table-depth There can be multiple queries in a report. This specifies how many queries deep you are. For instance, say you want to find every bookeq for every shiphead. Shiphead will be table-depth 1. Bookeq will be table-depth 2. depth-seq Ignore this. It should always be 1. You can use it to set up \u201cor\u201d-like behavior if you super seriously need to. table-name The table to query. \u201cshiphead\u201d, \u201cbookeq\u201d, etc. field-name The field to compare against. \u201ccreate-date\u201d, \u201ccontainer-number\u201d, etc. operator The verb to use. \u201c=\u201d, \u201cbegins\u201d, etc. field-value The value to compare against. If field-type is \u201cdate\u201d, you can use an integer. For instance, if you set field-value to \u201c-5\u201d, then it will be calculated as \u201ctoday - 5\u201d. If local-var is yes, then set this to the name of a local variable. For example, \u201cbranch\u201d. field-type The datatype to compare against. This is important for doing date/numeric comparisons. local-var If this is yes, then you will compare against a local variable as defined in the rpt-var table.","title":"rpt-qry"},{"location":"report/#rpt-var","text":"When running multi-table queries, you\u2019ll need some way to link one table to another. For example, to find every bookeq for every shiphead, you\u2019ll need to compare branch, dept, and id. rpt-var stores those as local variables at runtime. name Should match rpt-hdr.name. table-depth The depth to pull this variable from. Should match rpt-qry.table-depth. depth-seq Should match rpt-qry.depth-seq. field-name The field to pull as a local variable. For example, if you\u2019re pulling from shiphead, this might be \u201cbranch\u201d.","title":"rpt-var"},{"location":"report/#rpt-det","text":"Each rpt-det record defines a column on the report. name Should match rpt-hdr.name. field-seq The order of this column on the report. table-name The name of the table to pull this field from. For example, \u201cshiphead\u201d. field-name The name of the field to pull this data from. For example, \u201cbranch\u201d. col-name The name of the column to display on the spreadsheet. data-type The Progress datatype. excel-type The Excel datatype. This is important for formatting columns. calc-prog If you need to calculate this field, leave field-name blank. Then, set calc-prog to a program name. It will be called like this: run value(rpt-det.calc-prog) ( input rpt-hdr.name, input rpt-det.table-name, input dbc:get-curr-rowid(rpt-det.table-name), input rpt-det.col-name, input login, output rpt-cell.cell-value ) no-error. hidden Use this if you want to hide the value of this column. This is used when you want to pull different rows onto different tabs.","title":"rpt-det"},{"location":"report/#rpt-tab","text":"If you want more than one tab on your report, create an rpt-tab record. name Should match rpt-hdr.name. col-name The name of the column to generate tabs from. This should match rpt-det.col-name. cell-value If cell-value is blank, you will create one tab for every unique value in a column. If you want to filter to one value, specify it here.","title":"rpt-tab"},{"location":"report/#central-program","text":"Rashi 6 imp/report.p","title":"Central Program"},{"location":"report/#parameters","text":"Input Report time This determines the output location of the report. If it\u2019s a time (such as \u201c8AM\u201d, \u201c10AM\u201d, or \u201c1stOfMonth\u201d), it will end up in a directory where schedtask will generate the .xls at a given time. If it\u2019s blank, it will put the report in a user-specific directory based on the user login input. An email will be sent to {user login}@shapiro.com with realtime master XLS instructions. If it\u2019s \u201cshapiro360\u201d, it will go to a special directory for Shapiro 360 report downloads. See the documentation on shapiro360/runRpt.p for more information. Report name If this is blank, it will run all reports matching a given time. For example, if \u201c8AM\u201d is your report time, and the report name is blank, it will run all reports scheduled for 8 AM. User login If the report is being run on a schedule, then this should be \u201cSYSTEM\u201d. If it\u2019s run for an internal user, this should be their Automation login. (Hopefully we can just append \u201c@shapiro.com\u201d to get their email address.) If it\u2019s run for Shapiro 360, this will be their user ID in the Joomla MySQL database. In this case, it will be an integer.","title":"Parameters"},{"location":"report/#body","text":"Kicks off the reports based on your input parameters: whether it should be a scheduled job or an individual report, and whether it goes internal or out to Shapiro 360. This sets the output directory and calls wrt-rpt. It also emails the Automation user if necessary.","title":"Body"},{"location":"report/#internal-procedures","text":"There are two kinds of reports: internal and remote. Internal reports run a database query through imp/report.p. This largely happens in get-depth. External reports run a database query in an external program. imp/report.p will call this program. External reports are easier. Internal and external wrt-rpt Writes an individual report. If you\u2019re doing a scheduled run for a certain time, you\u2019ll call wrt-rpt once for each report scheduled. Calls prep-output to set up your individual directories and sendmail scripts. If reports run inside of imp/report.p, you\u2019ll call: get-depth to traverse the rpt-qry and rpt-det tables (these tables define the database queries and assign statements which produce reports) finalize-tabs to split data into tabs if necessary If reports don\u2019t run inside of imp/report.p, you\u2019ll call remote-report to run the report. procedure get-full-dir This will generate one directory for each rpt-xls record. There will be one by default. Internal reports get-depth This one\u2019s a doozy. This builds a query using rpt-qry to find records for pulling data onto the report. One rpt-qry record represents one phrase in the query\u2019s \u201cwhere\u201d clause. rpt-det specifies which data you pull onto the report. One rpt-det record represents one column on the report. Some notes on external programs: If rpt-hdr.calc-prog is set, get-depth runs a filter function like so: run value(rpt-hdr.calc-prog) ( input rpt-hdr.name, input table-name, input table-rowid, input \"filter\", input user-login, output continue ) no-error. Use this if you want to skip certain records, but can\u2019t do so through defining rpt-qry records. (For example, if you need logic involving \u201cor\u201d.) If continue is Y, then it will put this record on the report. Otherwise, it will skip this record. If rpt-det.calc-prog is blank, then it will pull the value out of the database directly. If rpt-det.calc-prog is set, it will run that program to determine the value to put in the cell. It calls rpt-det.calc-prog like so: run value(file-info:full-pathname) ( input rpt-hdr.name, input rpt-det.table-name, input dbc:get-curr-rowid(rpt-det.table-name), input rpt-det.col-name, input login, output rpt-cell.cell-value ) no-error. prep-output Prepare directories for CSV files to go in. Write rpt-list.txt so that the master XLS knows to run this report. Append to send-script.sh so that you mail the report out (if it\u2019s scheduled). procedure open-report Create directories to house .csv and .xls files. procedure finalize-tabs Write the header line in each .csv file you\u2019ve generated. procedure open-tab Not called. procedure output-line Outputs lines in every relevant directory. procedure print-line-by-tabs Outputs lines to every relevant .csv file. print-line-to-file Read the rpt-cell table and put data in a .csv file. External reports remote-report Calls a program to generate .csv files into a directory. Calls the program like so: run value(file-info:full-pathname) ( input rpt-hdr.name, input full-dir + \"/\", input login ) no-error.","title":"Internal Procedures"},{"location":"report/#examples-of-calls","text":"imp/report.p ( input \u201c7:30AM\u201d, input \u201c\u201d, input \u201cSYSTEM\u201d ). In this case, we\u2019re running all of the morning reports. imp/report.p ( input \u201cshapiro360\u201d, input \u201cfb-draft\u201d, input \u201c648\u201d ). A user on 360 is running the Five Below All PO report. The Joomla user ID is 648. imp/report.p ( input \u201crealtime\u201d, input \u201cmr-fb-pom\u201d, input \u201cgregory\u201d ). Automation user \u201cgregory\u201d is running the M+R Five Below POM report. Because the time is \u201crealtime\u201d, the realtime XLS will be emailed to \u201cgregory@shapiro.com\u201d. (It will be S:\\CORP-DAILY\\realtime\\gregory\\realtime-master.xls.) imp/report.p ( input \u201c\u201d, input \u201cfb-onwat-fb\u201d, input \u201cSYSTEM\u201d ). Someone is testing the Five Below On the Water report. They must have typed this into the Progress Editor. This run will end up in CORP-DAILY/rpt/realtime.","title":"Examples of Calls"},{"location":"report/#automation","text":"These are programs in Automation for updating and running reports. I have tried to put advice in the \u201cHelp\u201d box on the right in this program. Or rather, I have put advice in that box, and I have tried to make it good advice.","title":"Automation"},{"location":"report/#report-modifier","text":"Before starting, you must enter a report name in \u201cReport:\u201d and hit Enter. If you don\u2019t know one, hit F6 to look one up. Or, if you want to create a new report, type the name for it and hit Enter. Please make it program-friendly (alphanumerics, underscores, and dashes only). Use the \u201cRun\u201d button to run the report. It\u2019ll email you a realtime master XLS.","title":"Report Modifier"},{"location":"report/#header","text":"","title":"Header"},{"location":"report/#header_1","text":"Program If Use Remote? is yes, then Program will be called like this: run value(rpt-hdr.calc-prog) ( input rpt-hdr.name, input full-dir + \"/\", input login ) no-error. If Use Remote? is no, then Program will be used as a filter, like this: run value(rpt-hdr.calc-prog) ( input rpt-hdr.name, input table-name, input table-rowid, input \"filter\", input user-login, output continue ) no-error. Description A human-readable description for the program. Party The party for this program. You can leave it blank, set it to a customer code, or set it to a Shapiro department. Version A number representing the version of the program. Use remote? This determines the behavior of the Program field. If you set this to yes, then you can have most of the Progress programming coming out of your own program. Shapiro 360 Name If you want to display this in Shapiro 360, set this. The Party must match the customer code for a user. Shapiro 360 Users If you leave this blank, it\u2019ll display to all users matching the Party. If you set it, this report will only show to the users specified.","title":"Header"},{"location":"report/#input","text":"Field Name The name of the field. Make it nice to look at; do \u201cStart Date\u201d, not \u201cstart-date\u201d. Default Value The default value. This will be used if the report is run on a schedule. It will also auto-populate for a user before they have set their own preferred value. Data Type The datatype for this field. It\u2019s important to specify this so you can perform accurate numeric or date comparisons. Description You can use this to give instructions to the user if you need to.","title":"Input"},{"location":"report/#query","text":"You will only need this if \u201cUse Remote?\u201d is no.","title":"Query"},{"location":"report/#query_1","text":"Every record here defines a phrase in the \u201cwhere\u201d clause. Depth If you are querying tables within tables, this specifies which query you are talking about. For instance, say you are querying every bookeq for every shiphead. shiphead would be Depth 1. bookeq would be Depth 2. Table The table to query. Field The field to compare against. Operator The verb to use: \u201c=\u201d, \u201cbegins\u201d, etc. Value The value to compare against. If Data Type is \u201cdate\u201d, then you can put an integer here to perform comparisons relative to today. For example, if you enter \u201c-5\u201d, then the comparison will be against \u201ctoday - 5\u201d. Use Local Variable? Usually this is no. But, if you want to compare against the value of a local variable, select yes. If you select yes, then \u201cvalue\u201d must be the name of a local variable (\u201cbranch\u201d, \u201cdept\u201d, etc.). Note that you must define local variables below. Data Type The datatype of the field you\u2019re comparing against.","title":"Query"},{"location":"report/#local-variables","text":"Query Depth Which depth to pull this local variable from. If you\u2019re querying every bookeq for every shiphead: shiphead will be Depth 1. bookeq will be Depth 2. Field Name The field to pull into a local variable. In the shiphead/bookeq example: You\u2019ll want to pull \u201cbranch\u201d, \u201cdept\u201d, and \u201cid\u201d from Depth 1.","title":"Local Variables"},{"location":"report/#columns","text":"You will only need this if \u201cUse Remote?\u201d is no.","title":"Columns"},{"location":"report/#columns_1","text":"Move Up/Move Down Use these to change the order of columns on the report. The columns are printed in order from left to right. Table The table to pull this column from. Field The field name to pull this column from. Program If you need to calculate the value of this field, put in a program name. It will be called like this: run value(rpt-det.calc-prog) ( input rpt-hdr.name, input rpt-det.table-name, input dbc:get-curr-rowid(rpt-det.table-name), input rpt-det.col-name, input login, output rpt-cell.cell-value ) no-error. Column Header The title for the column to print in the spreadsheet. Hidden? Use this to hide a column. You\u2019ll want to do that if you are creating tabs. Progress Type The datatype of the variable in progress. Excel Type The format of the value in the Excel spreadsheet. This is important to set so that integers don\u2019t turn into scientific notation and so that dates don\u2019t get reformatted.","title":"Columns"},{"location":"report/#tabs","text":"Only bother with this if you want multiple tabs in your report. Column Header Set this to the Column Header from the above form. There will be one tab for each unique value in this column. For example, if you set it on the \u201cPort of Entry\u201d column, then you might have tabs like: \u201cLOS ANGELES, CA\u201d \u201cMEMPHIS, TN\u201d \u201cBALTIMORE, MD\u201d","title":"Tabs"},{"location":"report/#mail","text":"To Email The email to receive this report. From Email The email to send this report. Email Subject The subject line for the email. Email Body The body text for the email. If you leave it blank, we\u2019ll send a generic message. Send if Blank? If there\u2019s no report to send, then you can prevent us from sending anything. Set this to \u201cyes\u201d if you want the mailer to run even if we don\u2019t have a report. Run time The time of day to run. Run day The day of week to run. Run month The month to run. Leave blank if it should run every month. Column Name Use this to filter reports. Probably leave it blank. Cell Value Use this to filter reports. Probably leave it blank.","title":"Mail"},{"location":"report/#report-runner","text":"Search Type in a report name, party, or description. OK Hit this to select a report from the table. You need to hit \u201cOK\u201d before you can run it. Input This table displays user input fields. The program will set up defaults for you. You can change them using the \u201cInput Value\u201d text input. Input Value Click \u201cUpdate\u201d to change an input value. Click \u201cSave\u201d to save your new input value. Run This kicks off a report. Once it\u2019s done, you will receive an email with the report. Cancel You will need to wait while a report runs. If you cannot wait, you can click \u201cCancel\u201d. If you click \u201cCancel\u201d, your report will not finish. Return Use this to exit the Report Runner.","title":"Report Runner"},{"location":"report/#master-xls","text":"There are two different master XLSes. One is run during a scheduled run, and another is sent to users for running in real time. Folder structure: S:\\CORP-DAILY\\rpt{time that the report runs}{report name}{spreadsheet group}{CSV files} For example: S:\\CORP-DAILY\\rpt\\10am\\fb-draft\\rpt\\allpo.csv Each spreadsheet group will have one spreadsheet. Every file in the {CSV files} folder will be put as one tab on the same spreadsheet.","title":"Master XLS"},{"location":"report/#scheduled","text":"S:\\CORP-DAILY\\rpt\\master.xls auto_open() Get a list of report names to run out of rpt-list.txt. This goes in a variable called ReportNames. Each report name defines a directory to look in. For each directory, examine all sub-directories. This goes in SubFolder. For each subfolder, examine all CSV files. This goes in CSVNames. Run format_cells() for each entry in CSVNames. Each CSV file is a tab on the report. For each subfolder, save an XLS. Move all CSV files into the archive directory. Delete rpt-list.txt. Quit out of the Excel macro. format_cells() If this is a special tab (marked as \u201cPIE CHART\u201d or \u201cBAR GRAPH\u201d), pass it off to pie_chart() or bar_graph() as is appropriate. If you\u2019re not a special tab, run normally. Set the borders. Examine the header cell of each column. Apply formatting (bold text, gray background). Format the column if any formatting is specified. \u201c (DATE)\u201d \u2013 format as a date \u201c (INTEGER)\u201d \u2013 format as an integer \u201c{header};NumberFormat;{format}\u201d \u2013 format the column with whatever is in {format} Example: \u201cItem Price;NumberFormat;0.0000\u201d will apply number format \u201c0.0000\u201d to the column. The header will only say \u201cItem Price\u201d afterwards. For each cell, replace the text \u201c(NEWLINE)\u201d with an actual newline. Autofit and autofilter all cells. pie_chart() First row format: PIE CHART,{name},{number of rows} \u201cPIE CHART\u201d must be there to tell the macro that it\u2019s a pie chart. {name} is the name of your pie chart. {number of rows} is the number of rows of data you\u2019re sending. Then each subsequent row must have this format: {name},{value} {name} is the name of the pie chart slice. {value} is the value associated with the pie chart slice. bar_graph() First row format: BAR GRAPH,{name},{number of rows},{special},\u201dDATA LABELS\u201d \u201cBAR GRAPH\u201d must be there to tell the macro that it\u2019s a bar graph. {name} is the name of your bar graph. {number of rows} is the number of rows of data that you\u2019re sending. {special} is optional. It lets you format the bar graph. \u201cDOUBLE BAR\u201d \u2013 show columns clustered side by side \u201c100% STACK\u201d \u2013 show columns stacked on top one another to reach 100% If you want data labels, you must put \u201cDATA LABELS\u201d in the fifth spot. This is optional. The second row must have this format: ,{name 1},{name 2} The first spot must be blank. {name 1} is the label of each bar. For example: if the bars are labelled \u201cBrazil\u201d and \u201cTurkey\u201d, then {name 1} should be \u201cCountry\u201d. {name 2} is the value for each bar. For example: if the bars show the percentage of compliance, then {name 2} should be \u201c% compliant\u201d. Then each subsequent row must have this format: {label},{value} {label} is the name of a data point. {value} is the value for a data point. Col_Letter() Find the letter corresponding to a column. HexToLongRGB() Convert a hexadecimal color code to RGB.","title":"Scheduled"},{"location":"report/#realtime","text":"S:\\CORP-DAILY\\rpt\\realtime-master.xls This is the same as the scheduled XLS, except it keeps the report open so the user can read it.","title":"Realtime"},{"location":"report/#shapiro-360","text":"","title":"Shapiro 360"},{"location":"report/#report-getter","text":"shapiro360/getRpt.p This program checks if you\u2019re allowed to access a report. It also passes back any input fields for the report as defined in rpt-inp. This is so that custom-report.php can display input fields for the user to fill out. For instance, say your report has a start date and an end date; this will pass back two input fields, one for each date.","title":"Report Getter"},{"location":"report/#parameters_1","text":"Input web-login.user-login rpt-hdr.name Output Error message Human-readable report name A temp table of user input fields","title":"Parameters"},{"location":"report/#php-script","text":"custom-report.php","title":"PHP Script"},{"location":"report/#initialization","text":"A report name is passed in through $_GET[\u2018rpt\u2019]. This report name is passed into the report getter in order to draw the page. The report getter gives you the title to display on the page. It also gives you the input fields to display. For each row in ttInput, it draws an input field. If it\u2019s an integer, the page draws a number input; if it\u2019s a date, the page draws a date picker; otherwise, the page draws a regular text input.","title":"Initialization"},{"location":"report/#input-parameters","text":"This report reads the \u201crpt\u201d GET variable. This corresponds to a value in rpt-hdr.name.","title":"Input Parameters"},{"location":"report/#functions","text":"JavaScript window.addEventListener(\u2018load\u2019) Set the page name. If you have any date inputs, initialize the date picker. run_rpt() Pack up the input fields and call the report runner. This kicks off ping_file(). ping_file() Check to see if your report has finished running. If it\u2019s done, download it. If it\u2019s not done, check again in 1 second. This uses setInterval(). The report will be at https://shapiro360.shapiro.com/vdocs/reports/{user_id} {report name} {timestamp}.xlsx. However, ping_file() checks for a .txt file. This file is created once the .xlsx is completely written. This prevents the user from downloading a half-written spreadsheet. For big reports (Five Below All PO Report), this can run for like five minutes.","title":"Functions"},{"location":"report/#report-runner_1","text":"shapiro360/runRpt.p Take the user\u2019s input and create rpt-inp records. Then, run the report through imp/report.p. Once that is done, call the Master PY.","title":"Report Runner"},{"location":"report/#parameters_2","text":"Input web-login.user-login Joomla user ID rpt-hdr.name timestamp input temp-table Output Error message Location of the report This isn\u2019t used by the PHP script, since the web service will time out on larger reports. That\u2019s okay, because the Progress program will continue running, even though it will never return to the PHP script.","title":"Parameters"},{"location":"report/#internal-procedures_1","text":"crt-inp This creates rpt-inp records based on data the user passed in. This scheme allows you to save the user\u2019s input to the database so that it can be read from any program.","title":"Internal Procedures"},{"location":"report/#master-py","text":"shapiro360/openpyxl-2.5.4/report.py This uses the Python library openpyxl.","title":"Master PY"},{"location":"report/#input-parameters_1","text":"Input folder \u2013 folder to read CSVs from Output \u2013 the filename to save an XLSX into, minus the file extension","title":"Input Parameters"},{"location":"report/#body_1","text":"Read every CSV file out of the input folder one by one. Run normal_sheet(), pie_chart(), or bar_graph() as is appropriate. Save the output XLSX. Once that\u2019s done, save a TXT file to communicate that the XLSX is fully written.","title":"Body"},{"location":"report/#functions_1","text":"as_text() Return the value of a cell as a string. normal_sheet() Process a CSV file as a normal sheet. This mirrors format_cells() from the Master XLS. pie_chart() Process a CSV as a pie chart. This mirrors pie_chart() from the Master XLS. bar_graph() Process a CSV as a bar graph. This mirrors bar_graph() from the Master XLS.","title":"Functions"},{"location":"report/#scheduled-runs","text":"","title":"Scheduled Runs"},{"location":"report/#rashi-6-generate-csv-files","text":"The demon1 crontab runs /usr/bin/run_report_sched.sh runs on the 15th and 45th minute of each hour. That calls /usr/bin/mxp_report.pf. That calls imp/report-sched.p. imp/report-sched.p figures the time and passes it into imp/report.p. For instance, if the time is between 7:10 AM and 7:39 AM, imp/report-sched.p will pass \"8AM\" to imp/report.p.","title":"Rashi 6: Generate .csv Files"},{"location":"report/#schedtask-generate-xls-files","text":"The Task Scheduler calls C:\\bat\\report.bat on the 15th and 45th minute of each hour. It will determine what time to run. For instance, if the time is between 7:40 AM and 8:09 AM, it will get the time \"8AM\". It will look for a directory according to this time. This will be in the format: \\myshapiro\\dfs\\corp-daily\\rpt{TIME} If the directory exists, and it has a master XLS and rpt-list.txt, then report.bat will run the master XLS.","title":"schedtask: Generate .xls Files"},{"location":"report/#rashi-6-mail-xls-files","text":"The demon1 crontab runs /usr/bin/mail_report_sched.sh on the 0th and 30th minute of each hour. It will look for a mail script according to the time. The mail script will be in this location: /usr5/dailyreports/rpt/{TIME}/send-script.sh If the mail script is there, mail_report_sched.sh will run it.","title":"Rashi 6: Mail .xls Files"},{"location":"setup/","text":"Local Setup Download Git . Install Python (be sure to \"Add Python 3.7 to PATH\"). It should include pip by default. Go into your favorite directory. Right-click in whitespace and click \"Git Bash Here\". A command prompt will come up. Run the following: pip install mkdocs (this will install mkdocs, which you need to compile the documentation into HTML pages) git clone //shap-exp-360pro/c$/inetpub/wwwroot/shapiro-cms/templates/rt_metamorph/shapirodocumentaion cd shapirodocumentaion git checkout master Now make your changes. Once you are done, commit your raw changes to the master branch: git pull (make sure you have the latest version of the repository) git status (make sure your files show up as edited) git add {filename} (stage your files for a commit) git commit (be sure to add a commit message on the next screen, or else your commit will not take) git push (this pushes your raw changes to the master branch) mkdocs gh-deploy (this compiles the documentation site and syncs it to 360) Please remember to do both: push to the master branch and deploy the new site. These are two distinct branches, and updating one will not update the other.","title":"Local Setup"},{"location":"setup/#local-setup","text":"Download Git . Install Python (be sure to \"Add Python 3.7 to PATH\"). It should include pip by default. Go into your favorite directory. Right-click in whitespace and click \"Git Bash Here\". A command prompt will come up. Run the following: pip install mkdocs (this will install mkdocs, which you need to compile the documentation into HTML pages) git clone //shap-exp-360pro/c$/inetpub/wwwroot/shapiro-cms/templates/rt_metamorph/shapirodocumentaion cd shapirodocumentaion git checkout master Now make your changes. Once you are done, commit your raw changes to the master branch: git pull (make sure you have the latest version of the repository) git status (make sure your files show up as edited) git add {filename} (stage your files for a commit) git commit (be sure to add a commit message on the next screen, or else your commit will not take) git push (this pushes your raw changes to the master branch) mkdocs gh-deploy (this compiles the documentation site and syncs it to 360) Please remember to do both: push to the master branch and deploy the new site. These are two distinct branches, and updating one will not update the other.","title":"Local Setup"},{"location":"Document Indexing/about/","text":"Document Indexing Programs General Overview Each index has their own specific rules for matching documents to Files/Customers/Parts. Shapiro360 and GLG docs must be placed in their intended CustNo folder with a FileID as the prefix in order to fully match. Customer and Profile docs simply need to be in the right CustNo folder, but its suggested to use the CustNo as the prefix. FPPI documents are specialty Customer Docs that need be in the right AgentNo folder and should follow the naming convention; AgentNo-FPPI-DocumentName.Ext . Classification Docs simply need to be in a the right Class PartNo folder. That's about it. Damco Docs & MR Docs are both automated and explained in further detail below. Regardless of Index type, we reject the following extensions; MSG TMP URL ZIP RAR HTML XML LNK DB We also reject documents that begin with ~$ since these are temporary files. But honestly, people need to stop trying to edit files in the Upload Directory. Editing should all occur BEFORE entering the Upload Directory. Not that it will stop people. /shrug Error Logs We keep track of error and success(history) logs in \\\\shapiro-fs01\\DAILY\\docindexer . Upload/Storage Folder Locations Windows Paths Document Type Upload Folder Secure Storage Shapiro360/ScannedDocs \\\\shapiro-fs01\\DOC-Index\\WEB-SCANNEDDOCS \\\\shapiro-fs01\\WEB\\Scanneddocs GLG/TransDocs \\\\shapiro-fs01\\DOC-Index\\WEB-GLGDOCS \\\\shapiro-fs01\\GLG-DOCS CustomerDocs \\\\shapiro-fs01\\DOC-Index\\WEB-CUSTOMERDOCS \\\\shapiro-fs01\\WEB\\CustomerDocs Profile Docs \\\\shapiro-fs01\\DOC-Index\\WEB-PROFILES \\\\shapiro-fs01\\CORP-COMMON\\Profiles Classification Docs \\\\shapiro-fs01\\DOC-Index\\WEB-ClASSDOCS \\\\shapiro-fs01\\WEB\\ClassDocs Damco Docs \\\\shapiro-fs01\\DOC-Index\\DAMCODOCS \\\\shapiro-fs01\\WEB\\ClassDocs MR Docs \\\\shapiro-fs01\\DOC-Index\\MRDOCS \\\\shapiro-fs01\\WEB\\ClassDocs Rashi Paths Document Type Upload Folder Secure Storage Shapiro360/ScannedDocs usr5/shapiro360docs usr5/doc-index/WEB-SCANNEDDOCS GLG/TransDocs usr5/glg-docs usr5/doc-index/WEB-GLGDOCS CustomerDocs usr5/webcustomerdocs usr5/doc-index/WEB-CUSTOMERDOCS Profile Docs usr5/webprofiles usr5/doc-index/WEB-PROFILES Classification Docs usr5/webclassdocs usr5/doc-index/WEB-CLASSDOCS Damco Docs usr5/ftproot/mrspedag usr5/doc-index/DAMCODOCS MR Docs usr5/ftproot/mrspedag/mrspedag/docs usr5/doc-index/MRDOCS Main Indexer imp/docindexer.p cd /progs/mxp/mxp81e/shapsrc/imp/docindexer.p imp/mrdocs.p cd /progs/mxp/mxp81e/shapsrc/imp/mrdocs.p mrdocs.p will move Zip files into the DOC-Index\\MRDOCS\\0-PPROCESSING\\ folder to perform the matching and renaming, then move the completed files into the parent folder, DOC-Index\\MRDOCS , to queue them up for proper Indexing. Unmatched Zips will move to DOC-Index\\MRDOCS\\0-UNMATCHED\\ , where they will sit until they are attempted to match again every other hour by /usr5/bin/rerun-nomatch.sh . This is to try and catch Zips that failed a match because they arrived before the File could be created. imp/damcodocs.p cd /progs/mxp/mxp81e/shapsrc/imp/damcodocs.p Low maintenance and typically smooth. More info will come if needed.","title":"Document Indexing Programs"},{"location":"Document Indexing/about/#document-indexing-programs","text":"","title":"Document Indexing Programs"},{"location":"Document Indexing/about/#general-overview","text":"Each index has their own specific rules for matching documents to Files/Customers/Parts. Shapiro360 and GLG docs must be placed in their intended CustNo folder with a FileID as the prefix in order to fully match. Customer and Profile docs simply need to be in the right CustNo folder, but its suggested to use the CustNo as the prefix. FPPI documents are specialty Customer Docs that need be in the right AgentNo folder and should follow the naming convention; AgentNo-FPPI-DocumentName.Ext . Classification Docs simply need to be in a the right Class PartNo folder. That's about it. Damco Docs & MR Docs are both automated and explained in further detail below. Regardless of Index type, we reject the following extensions; MSG TMP URL ZIP RAR HTML XML LNK DB We also reject documents that begin with ~$ since these are temporary files. But honestly, people need to stop trying to edit files in the Upload Directory. Editing should all occur BEFORE entering the Upload Directory. Not that it will stop people. /shrug","title":"General Overview"},{"location":"Document Indexing/about/#error-logs","text":"We keep track of error and success(history) logs in \\\\shapiro-fs01\\DAILY\\docindexer .","title":"Error Logs"},{"location":"Document Indexing/about/#uploadstorage-folder-locations","text":"Windows Paths Document Type Upload Folder Secure Storage Shapiro360/ScannedDocs \\\\shapiro-fs01\\DOC-Index\\WEB-SCANNEDDOCS \\\\shapiro-fs01\\WEB\\Scanneddocs GLG/TransDocs \\\\shapiro-fs01\\DOC-Index\\WEB-GLGDOCS \\\\shapiro-fs01\\GLG-DOCS CustomerDocs \\\\shapiro-fs01\\DOC-Index\\WEB-CUSTOMERDOCS \\\\shapiro-fs01\\WEB\\CustomerDocs Profile Docs \\\\shapiro-fs01\\DOC-Index\\WEB-PROFILES \\\\shapiro-fs01\\CORP-COMMON\\Profiles Classification Docs \\\\shapiro-fs01\\DOC-Index\\WEB-ClASSDOCS \\\\shapiro-fs01\\WEB\\ClassDocs Damco Docs \\\\shapiro-fs01\\DOC-Index\\DAMCODOCS \\\\shapiro-fs01\\WEB\\ClassDocs MR Docs \\\\shapiro-fs01\\DOC-Index\\MRDOCS \\\\shapiro-fs01\\WEB\\ClassDocs Rashi Paths Document Type Upload Folder Secure Storage Shapiro360/ScannedDocs usr5/shapiro360docs usr5/doc-index/WEB-SCANNEDDOCS GLG/TransDocs usr5/glg-docs usr5/doc-index/WEB-GLGDOCS CustomerDocs usr5/webcustomerdocs usr5/doc-index/WEB-CUSTOMERDOCS Profile Docs usr5/webprofiles usr5/doc-index/WEB-PROFILES Classification Docs usr5/webclassdocs usr5/doc-index/WEB-CLASSDOCS Damco Docs usr5/ftproot/mrspedag usr5/doc-index/DAMCODOCS MR Docs usr5/ftproot/mrspedag/mrspedag/docs usr5/doc-index/MRDOCS","title":"Upload/Storage Folder Locations"},{"location":"Document Indexing/about/#main-indexer","text":"imp/docindexer.p cd /progs/mxp/mxp81e/shapsrc/imp/docindexer.p imp/mrdocs.p cd /progs/mxp/mxp81e/shapsrc/imp/mrdocs.p mrdocs.p will move Zip files into the DOC-Index\\MRDOCS\\0-PPROCESSING\\ folder to perform the matching and renaming, then move the completed files into the parent folder, DOC-Index\\MRDOCS , to queue them up for proper Indexing. Unmatched Zips will move to DOC-Index\\MRDOCS\\0-UNMATCHED\\ , where they will sit until they are attempted to match again every other hour by /usr5/bin/rerun-nomatch.sh . This is to try and catch Zips that failed a match because they arrived before the File could be created. imp/damcodocs.p cd /progs/mxp/mxp81e/shapsrc/imp/damcodocs.p Low maintenance and typically smooth. More info will come if needed.","title":"Main Indexer"},{"location":"Hub360/about/","text":"Document Hub 360 << back Overview Built w/ React, the Shapiro Document Portal, aka Hub360, is a one page app that allows the user to easily download, rename, and upload documents to/from the: shapiro360-docs-index, glg-docs-index, customer-docs-index ( General ) customer-docs-index ( FPPI Authorization ) Developer's Documentation User Guide","title":"Document Hub 360"},{"location":"Hub360/about/#document-hub-360","text":"<< back","title":"Document Hub 360"},{"location":"Hub360/about/#overview","text":"Built w/ React, the Shapiro Document Portal, aka Hub360, is a one page app that allows the user to easily download, rename, and upload documents to/from the: shapiro360-docs-index, glg-docs-index, customer-docs-index ( General ) customer-docs-index ( FPPI Authorization )","title":"Overview"},{"location":"Hub360/about/#developers-documentation","text":"","title":"Developer's Documentation"},{"location":"Hub360/about/#user-guide","text":"","title":"User Guide"},{"location":"Hub360/dev-documentation/","text":"Developer Documentation << back Local Setup If you don't have Git, download it here git clone //shap-exp-360pro/c$/inetpub/wwwroot/shapiro-apps/hub360repo cd hub360repo and Install the package.json with npm install to procure all required Node_Modules. When ready to development/testing, npm start to spin up a local server with a development build. It has a hot-reload so every save will re-compile and refresh the browser window. This app also comes bundled with SASS so we won't be relying on classic css. Note : At the time of writing, WSDL calls do not allow for CORS access so will all fail. The only way to test WSDL calls is to get a production build up on the shapiro360 server. Error Logs cd /usr5/dailyreports/kkim/0-logs/hub360/downloads // Monthly cd /usr5/dailyreports/kkim/0-logs/hub360/renames // Monthly cd /usr5/dailyreports/kkim/0-logs/hub360/uploads // Daily Production Build Setup At the time of writing, pushing a production build requires manual editing. After running npm run build , drag the files from the build folder out to a folder on the Shapiro360 server of your choice. In my examples, I use /inetpub/wwwroot/shapiro-cms/hub360react/ . Afterwards, you'll need to edit the index.html to swap out all paths from /static/ with /hub360react/static/ or whatever folder. This would be reachable via https://shapiro360.shapiro.com/hub360react/ , or replacing hub360react with whatever folder. React Parent-Child Relationships ../src/App.jsx is the main parent component. It contains the initial state and defines. ../src/components/ holds all of the child components such as SearchForm.jsx , DocList.jsx , and so on. Note: When you need a child component to affect the state of its sibling or parent, you'll need to pass the values to the parent first. Everything is one-way parent-to-child inheritance based. App.jsx class App extends Component { constructor(props) { super(props); this.state = initialState; // Search Bar Input this.handleInputChange = this.handleInputChange.bind(this); ... } handleInputChange(e) { e.preventDefault(); let validCheck = 'form--invalid'; // Reject Alphabet/special chars. // ShapiroIDs definitely only use numbers. if( e.target.value.match('^[0-9]+$') || e.target.value === '' ) { validCheck = 'form--valid'; // Must be numerical chars only. if( e.target.value.length < 7 ) { validCheck = 'form--incomplete'; } // Must be 7 chars before submit } this.setState({ fileID: e.target.value, fileID_Valid: validCheck, }) } ... render(){ ... return( <SearchForm passHandleInputChange = {this.handleInputChange} //... /> ... ) } In the App.jsx file, we've first bound the function to this component, then defined it, then passed it into the SearchForm React element. We also perform a this.setState() within the handleInputChange() define. This will update the App State, which will cause a re-render of App and all child components. By passing {this.handleInputChange} into SearchForm we are sending the function as a prop named passHandleInputChange . Also notice that we use CapitalCamelCase for React elements. This is mandatory. SearchForm.jsx class SearchForm extends Component { ... render(){ ... return ( <div> <form> <div className=\"form-inline\"> <input placeholder=\"Shapiro File ID\" className=\"form-control input_bar\" maxLength=\"7\" onChange={this.props.passHandleInputChange} //... />; ... </div> </form> </div> ) } In SearchForm.jsx , we are attaching the previously passed passHandleInputChange prop to an onChange event for the input element as {this.props.passHandleInputChange} . Every change in this input field will trigger the handleInputChange() function, which ends in a setState that triggers a re-render of the App component, which subsequently triggers a re-render on all of its child components, SearchForm included. This is the same process for getting a child component to affect a sibling component. React Conditional Piece-Meal Rendering Hub360 is written in JSX, which allows for html-like and css-like language, but at the end of the day, still behaves as Javascript. The below is basic example of using js' conditional to alter how React will render the html-like language. ColorKey.jsx export default class ColorKey extends React.Component { render() { let warningText = \" - Document name can't match to File. File ID will be auto-added on upload.\"; if ( this.props.indexMode === \"custdocs\" ) { warningText = \" - Document name can't match to Customer. CustNo will be auto-added on upload.\" } else if ( this.props.indexMode === \"fppidocs\" ) { warningText = \" - Document name can't match to Agent-FPPI or is missing -FPPI- tag. Hub360 will auto add the Agent and FPPI tags. \" } return ( <div className=\"row upload__key\"> ... <div className=\"col-12 col-md-4\"> {warningText} </div> ... </div> ); } } This can apply for entire elements as well; DocList_Controller.jsx export default class Controller extends React.Component { render() { let downloadButton = <button className = \"btn btn-primary\" disabled > <i className=\"fa fa-download\"></i>&nbsp;Download </button>; if ( this.props.hasSelection === true || this.props.selectAll === true ) { downloadButton = <button className = \"btn btn-primary\" onClick = {this.props.passHandleZipGen('selected')}> <i className=\"fa fa-download\"></i>&nbsp;Download </button>; } return ( <div className=\"doclist__controller\"> {downloadButton} </div> ) } } Keep in mind that JSX can only recognize one element at a time however. Valid Example: downloadButton = <button>...</button> Invalid: downloadButton = <button>...</button> <span>...</span> But your single element can contain as many child elements as you wish; downloadButton = <button> <span> <i>...</i> </span> <span>...</span> </button>","title":"Developer Documentation"},{"location":"Hub360/dev-documentation/#developer-documentation","text":"<< back","title":"Developer Documentation"},{"location":"Hub360/dev-documentation/#local-setup","text":"","title":"Local Setup"},{"location":"Hub360/dev-documentation/#if-you-dont-have-git-download-it-here","text":"git clone //shap-exp-360pro/c$/inetpub/wwwroot/shapiro-apps/hub360repo cd hub360repo and Install the package.json with npm install to procure all required Node_Modules. When ready to development/testing, npm start to spin up a local server with a development build. It has a hot-reload so every save will re-compile and refresh the browser window. This app also comes bundled with SASS so we won't be relying on classic css. Note : At the time of writing, WSDL calls do not allow for CORS access so will all fail. The only way to test WSDL calls is to get a production build up on the shapiro360 server.","title":"If you don't have Git, download it here"},{"location":"Hub360/dev-documentation/#error-logs","text":"cd /usr5/dailyreports/kkim/0-logs/hub360/downloads // Monthly cd /usr5/dailyreports/kkim/0-logs/hub360/renames // Monthly cd /usr5/dailyreports/kkim/0-logs/hub360/uploads // Daily","title":"Error Logs"},{"location":"Hub360/dev-documentation/#production-build-setup","text":"At the time of writing, pushing a production build requires manual editing. After running npm run build , drag the files from the build folder out to a folder on the Shapiro360 server of your choice. In my examples, I use /inetpub/wwwroot/shapiro-cms/hub360react/ . Afterwards, you'll need to edit the index.html to swap out all paths from /static/ with /hub360react/static/ or whatever folder. This would be reachable via https://shapiro360.shapiro.com/hub360react/ , or replacing hub360react with whatever folder.","title":"Production Build Setup"},{"location":"Hub360/dev-documentation/#react-parent-child-relationships","text":"../src/App.jsx is the main parent component. It contains the initial state and defines. ../src/components/ holds all of the child components such as SearchForm.jsx , DocList.jsx , and so on. Note: When you need a child component to affect the state of its sibling or parent, you'll need to pass the values to the parent first. Everything is one-way parent-to-child inheritance based. App.jsx class App extends Component { constructor(props) { super(props); this.state = initialState; // Search Bar Input this.handleInputChange = this.handleInputChange.bind(this); ... } handleInputChange(e) { e.preventDefault(); let validCheck = 'form--invalid'; // Reject Alphabet/special chars. // ShapiroIDs definitely only use numbers. if( e.target.value.match('^[0-9]+$') || e.target.value === '' ) { validCheck = 'form--valid'; // Must be numerical chars only. if( e.target.value.length < 7 ) { validCheck = 'form--incomplete'; } // Must be 7 chars before submit } this.setState({ fileID: e.target.value, fileID_Valid: validCheck, }) } ... render(){ ... return( <SearchForm passHandleInputChange = {this.handleInputChange} //... /> ... ) } In the App.jsx file, we've first bound the function to this component, then defined it, then passed it into the SearchForm React element. We also perform a this.setState() within the handleInputChange() define. This will update the App State, which will cause a re-render of App and all child components. By passing {this.handleInputChange} into SearchForm we are sending the function as a prop named passHandleInputChange . Also notice that we use CapitalCamelCase for React elements. This is mandatory. SearchForm.jsx class SearchForm extends Component { ... render(){ ... return ( <div> <form> <div className=\"form-inline\"> <input placeholder=\"Shapiro File ID\" className=\"form-control input_bar\" maxLength=\"7\" onChange={this.props.passHandleInputChange} //... />; ... </div> </form> </div> ) } In SearchForm.jsx , we are attaching the previously passed passHandleInputChange prop to an onChange event for the input element as {this.props.passHandleInputChange} . Every change in this input field will trigger the handleInputChange() function, which ends in a setState that triggers a re-render of the App component, which subsequently triggers a re-render on all of its child components, SearchForm included. This is the same process for getting a child component to affect a sibling component.","title":"React Parent-Child Relationships"},{"location":"Hub360/dev-documentation/#react-conditional-piece-meal-rendering","text":"Hub360 is written in JSX, which allows for html-like and css-like language, but at the end of the day, still behaves as Javascript. The below is basic example of using js' conditional to alter how React will render the html-like language. ColorKey.jsx export default class ColorKey extends React.Component { render() { let warningText = \" - Document name can't match to File. File ID will be auto-added on upload.\"; if ( this.props.indexMode === \"custdocs\" ) { warningText = \" - Document name can't match to Customer. CustNo will be auto-added on upload.\" } else if ( this.props.indexMode === \"fppidocs\" ) { warningText = \" - Document name can't match to Agent-FPPI or is missing -FPPI- tag. Hub360 will auto add the Agent and FPPI tags. \" } return ( <div className=\"row upload__key\"> ... <div className=\"col-12 col-md-4\"> {warningText} </div> ... </div> ); } } This can apply for entire elements as well; DocList_Controller.jsx export default class Controller extends React.Component { render() { let downloadButton = <button className = \"btn btn-primary\" disabled > <i className=\"fa fa-download\"></i>&nbsp;Download </button>; if ( this.props.hasSelection === true || this.props.selectAll === true ) { downloadButton = <button className = \"btn btn-primary\" onClick = {this.props.passHandleZipGen('selected')}> <i className=\"fa fa-download\"></i>&nbsp;Download </button>; } return ( <div className=\"doclist__controller\"> {downloadButton} </div> ) } } Keep in mind that JSX can only recognize one element at a time however. Valid Example: downloadButton = <button>...</button> Invalid: downloadButton = <button>...</button> <span>...</span> But your single element can contain as many child elements as you wish; downloadButton = <button> <span> <i>...</i> </span> <span>...</span> </button>","title":"React Conditional Piece-Meal Rendering"},{"location":"Hub360/user-guide/","text":"User Guide << back General Guide The Hub will always default to filtering for Shapiro360 documents. This can be changed via the dropdown to filter through GLG/Transdocs , Customer Docs , and FPPI Authorizations . When searching for a File, a valid File ID must be provided. When filtering for a Customer Doc or an FPPI Authorization however, you must use a valid CustNo or AgentNo . File/Customer Detail Shapiro360/GLG docs will return a File Details bar that contains CustNo, File Types, Acct Coordinators, etc. For Customer Docs/FPPI Authorizations, you'll receive a Customer Detail bar that will display phone numbers, address, Consignees, etc. Downloading & Renaming existing Documents If documents are found, you can click on individual names to preview the doc, or select any number of documents via checkbox to download a .ZIP of the documents. To rename a document, click the \"Edit Mode\" at the top right of the listing. When complete, exit the edit mode and click \"Save\" at the bottom right. Uploading New Documents You can either click the area within the orange outline or drag files directly into it to begin processing documents. To release these documents to upload a different batch, click Start Over on the bottom left of the Upload listing. Documents will highlight in either red, yellow, or green. Red - Danger, this document has the same name as one already uploaded. If you do nothing, Hub360 will overwrite the old one. Yellow - Warning, this document's name doesn't quite match the naming convention. If you do nothing, Hub360 will rename the file on your behalf to match. Green - Accepted, no problems detected Document Names can be edited by clicking the Edit Mode button on the top-left of the Upload Listing. Documents with an unaccepted extension ( eg. .MSG, .HTML, .XYZ ), will automatically be rejected and listed below.","title":"User Guide"},{"location":"Hub360/user-guide/#user-guide","text":"<< back","title":"User Guide"},{"location":"Hub360/user-guide/#general-guide","text":"The Hub will always default to filtering for Shapiro360 documents. This can be changed via the dropdown to filter through GLG/Transdocs , Customer Docs , and FPPI Authorizations . When searching for a File, a valid File ID must be provided. When filtering for a Customer Doc or an FPPI Authorization however, you must use a valid CustNo or AgentNo .","title":"General Guide"},{"location":"Hub360/user-guide/#filecustomer-detail","text":"Shapiro360/GLG docs will return a File Details bar that contains CustNo, File Types, Acct Coordinators, etc. For Customer Docs/FPPI Authorizations, you'll receive a Customer Detail bar that will display phone numbers, address, Consignees, etc.","title":"File/Customer Detail"},{"location":"Hub360/user-guide/#downloading-renaming-existing-documents","text":"If documents are found, you can click on individual names to preview the doc, or select any number of documents via checkbox to download a .ZIP of the documents. To rename a document, click the \"Edit Mode\" at the top right of the listing. When complete, exit the edit mode and click \"Save\" at the bottom right.","title":"Downloading &amp; Renaming existing Documents"},{"location":"Hub360/user-guide/#uploading-new-documents","text":"You can either click the area within the orange outline or drag files directly into it to begin processing documents. To release these documents to upload a different batch, click Start Over on the bottom left of the Upload listing. Documents will highlight in either red, yellow, or green. Red - Danger, this document has the same name as one already uploaded. If you do nothing, Hub360 will overwrite the old one. Yellow - Warning, this document's name doesn't quite match the naming convention. If you do nothing, Hub360 will rename the file on your behalf to match. Green - Accepted, no problems detected Document Names can be edited by clicking the Edit Mode button on the top-left of the Upload Listing. Documents with an unaccepted extension ( eg. .MSG, .HTML, .XYZ ), will automatically be rejected and listed below.","title":"Uploading New Documents"},{"location":"Ocean Insights/about/","text":"Ocean Insights Test","title":"About"}]}